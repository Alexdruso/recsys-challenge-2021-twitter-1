{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NVTabular_Preprocessing_Analysis_tweet_id_clean.ipynb","provenance":[{"file_id":"18D8R285BXjfzeMYLn1ZfHInriGw7APf5","timestamp":1618786584249},{"file_id":"17kM_qMpdbSCnrGTLHzuLAf3Yn0_5Rhoa","timestamp":1618763137123},{"file_id":"1Cjz_EzeIYwEnbH-pTwKES_ZdibqF4ego","timestamp":1617570254639}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"94Fdr-lt6wGv"},"source":["# Install RAPIDS (takes ~10 min).\n","!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n","!bash rapidsai-csp-utils/colab/rapids-colab.sh 0.18\n","\n","import sys, os\n","\n","dist_package_index = sys.path.index('/usr/local/lib/python3.7/dist-packages')\n","sys.path = sys.path[:dist_package_index] + ['/usr/local/lib/python3.7/site-packages'] + sys.path[dist_package_index:]\n","sys.path\n","exec(open('rapidsai-csp-utils/colab/update_modules.py').read(), globals())\n","\n","# https://github.com/NVIDIA/NVTabular/blob/main/examples/winning-solution-recsys2020-twitter/01-02-04-Download-Convert-ETL-with-NVTabular-Training-with-XGBoost.ipynb     "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rx8doh2MmYN6"},"source":["# Needed to fix conda and install nvtabular.\n","!conda install https://repo.anaconda.com/pkgs/main/linux-64/conda-4.9.2-py37h06a4308_0.tar.bz2\n","!pip install git+https://github.com/NVIDIA/NVTabular.git@main"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"okhsqbb5KJR6"},"source":["# For rapidsai 0.19 ONLY, not working.\n","\"\"\"\n","!sudo add-apt-repository ppa:ubuntu-toolchain-r/test\n","\n","!sudo apt-get update\n","\n","!sudo apt-get install gcc-4.9\n","\n","!sudo apt-get upgrade libstdc++6\n","!sudo apt-get dist-upgrade\n","!strings /usr/lib/x86_64-linux-gnu/libstdc++.so.6 | grep GLIBCXX\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4fjfRlBpzWzo"},"source":["# External Dependencies\n","import time\n","import glob\n","import gc\n","\n","import cupy as cp          # CuPy is an implementation of NumPy-compatible multi-dimensional array on GPU\n","import cudf                # cuDF is an implementation of Pandas-like Dataframe on GPU\n","import rmm                 # library for pre-allocating memory on GPU\n","import dask                # dask is an open-source library to nateively scale Python on multiple workers/nodes\n","import dask_cudf           # dask_cudf uses dask to scale cuDF dataframes on multiple workers/nodes\n","\n","import numpy as np\n","# NVTabular is the core library, we will use here for feature engineering/preprocessing on GPU\n","import nvtabular as nvt\n","import xgboost as xgb\n","\n","# More dask / dask_cluster related libraries to scale NVTabular\n","from dask_cuda import LocalCUDACluster\n","from dask.distributed import Client\n","from dask.distributed import wait\n","from dask.utils import parse_bytes\n","from dask.delayed import delayed\n","from nvtabular.utils import device_mem_size\n","from nvtabular.column_group import ColumnGroup"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p3FWbQ3v0Wwm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618076905651,"user_tz":-120,"elapsed":387,"user":{"displayName":"Giacomo Lodigiani","photoUrl":"","userId":"15410861073597855322"}},"outputId":"6e028a4a-bfc4-45c9-a71f-2fe2e556aca7"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sat Apr 10 17:48:25 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P0    28W / 250W |      2MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lIgkj2ZeXeAF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618738414599,"user_tz":-120,"elapsed":37426,"user":{"displayName":"Giacomo Lodigiani","photoUrl":"","userId":"15410861073597855322"}},"outputId":"0e5c97a0-d28e-4306-927e-0686cc95da54"},"source":["# Assume dataset in MyDrive/RecSys2021\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","BASE_DIR = '/content/drive/MyDrive/RecSys2021/original_dataset'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yAqrZ9mA9qOO","executionInfo":{"status":"ok","timestamp":1618762415409,"user_tz":-120,"elapsed":23887637,"user":{"displayName":"Giacomo Lodigiani","photoUrl":"","userId":"15410861073597855322"}},"outputId":"a91c5541-f815-41fa-8e15-e2a7104784a7"},"source":["# One BIG loop\n","\n","import sys\n","import json\n","import shutil\n","\n","gc.collect()\n","if (os.path.isdir('preprocess_out_1/')):\n","      shutil.rmtree('preprocess_out_1/')\n","if (os.path.isdir('preprocess_out_2/')):\n","      shutil.rmtree('preprocess_out_2/')\n","\n","features = [\n","    'text_tokens',    ###############\n","    'hashtags',       #Tweet Features\n","    'tweet_id',       #\n","    'media',          #\n","    'links',          #\n","    'domains',        #\n","    'tweet_type',     #\n","    'language',       #\n","    'timestamp',      ###############\n","    'a_user_id',              ###########################\n","    'a_follower_count',       #Engaged With User Features\n","    'a_following_count',      #\n","    'a_is_verified',          #\n","    'a_account_creation',     ###########################\n","    'b_user_id',              #######################\n","    'b_follower_count',       #Engaging User Features\n","    'b_following_count',      #\n","    'b_is_verified',          #\n","    'b_account_creation',     #######################\n","    'b_follows_a',    #################### Engagement Features\n","    'reply',          #Target Reply\n","    'retweet',        #Target Retweet    \n","    'retweet_comment',#Target Retweet with comment\n","    'like',           #Target Like\n","                      ####################\n","]\n","\n","\n","tweet_id_dict = {}\n","tweet_id_dict['d_size'] = 0\n","media_dict = {}\n","media_dict['d_size'] = 0\n","hashtag_dict = {}\n","hashtag_dict['d_size'] = 0\n","link_dict = {}\n","link_dict['d_size'] = 0\n","domain_dict = {}\n","domain_dict['d_size'] = 0\n","lang_dict = {}\n","lang_dict['d_size'] = 0\n","tweet_type_dict = {}\n","tweet_type_dict['d_size'] = 0\n","user_dict = {}\n","user_dict['d_size'] = 0\n","\n","\n","# Check if value already mapped, if not map it\n","def add_dict_media(el):\n","    if (len(el.split('$$$$')) == 2):\n","        return int(el.split('$$$$')[1])\n","    else:\n","        media_dict[el] = media_dict['d_size']\n","        media_dict['d_size'] = media_dict['d_size'] + 1\n","        return media_dict['d_size'] - 1\n","def add_dict_hashtag(el):\n","    if (len(el.split('$$$$')) == 2):\n","        return int(el.split('$$$$')[1])\n","    else:\n","        hashtag_dict[el] = hashtag_dict['d_size']\n","        hashtag_dict['d_size'] = hashtag_dict['d_size'] + 1\n","        return hashtag_dict['d_size'] - 1\n","def add_dict_link(el):\n","    if (len(el.split('$$$$')) == 2):\n","        return int(el.split('$$$$')[1])\n","    else:\n","        link_dict[el] = link_dict['d_size']\n","        link_dict['d_size'] = link_dict['d_size'] + 1\n","        return link_dict['d_size'] - 1\n","def add_dict_domain(el):\n","    if (len(el.split('$$$$')) == 2):\n","        return int(el.split('$$$$')[1])\n","    else:\n","        domain_dict[el] = domain_dict['d_size']\n","        domain_dict['d_size'] = domain_dict['d_size'] + 1\n","        return domain_dict['d_size'] - 1\n","def add_dict_lang(el):\n","    if (len(el.split('$$$$')) == 2):\n","        return int(el.split('$$$$')[1])\n","    else:\n","        lang_dict[el] = lang_dict['d_size']\n","        lang_dict['d_size'] = lang_dict['d_size'] + 1\n","        return lang_dict['d_size'] - 1\n","def add_dict_tweet_type(el):\n","    if (len(el.split('$$$$')) == 2):\n","        return int(el.split('$$$$')[1])\n","    else:\n","        tweet_type_dict[el] = tweet_type_dict['d_size']\n","        tweet_type_dict['d_size'] = tweet_type_dict['d_size'] + 1\n","        return tweet_type_dict['d_size'] - 1\n","def add_dict_tweet_id(el):\n","    if (len(el.split('$$$$')) == 2):\n","        return int(el.split('$$$$')[1])\n","    else:\n","        tweet_id_dict[el] = tweet_id_dict['d_size']\n","        tweet_id_dict['d_size'] = tweet_id_dict['d_size'] + 1\n","        return tweet_id_dict['d_size'] - 1\n","def add_dict_user(el):\n","    if (len(el.split('$$$$')) == 2):\n","        return int(el.split('$$$$')[1])\n","    else:\n","        user_dict[el] = user_dict['d_size']\n","        user_dict['d_size'] = user_dict['d_size'] + 1\n","        return user_dict['d_size'] - 1\n","\n","# Change to map other features\n","# Note: only tweet_id need else, other features can be done one shot\n","def simple_map(df):\n","    if (j < end_dict):\n","        df['tweet_id'] = df['tweet_id'].map(lambda x: tweet_id_dict[x] if x in tweet_id_dict else add_dict_tweet_id(x))\n","    else:\n","        # When dictionary is full, don't add mappings, just replace if label already mapped\n","        df['tweet_id'] = df['tweet_id'].map(lambda x: str('value$$$$' + str(tweet_id_dict[x])) if x in tweet_id_dict else x)\n","    #df['media'] = df['media'].map(lambda x: media_dict[x] if x in media_dict else add_dict_media(x))\n","    #df['hashtags'] = df['hashtags'].map(lambda x: hashtag_dict[x] if x in hashtag_dict else add_dict_hashtag(x))\n","    #df['links'] = df['links'].map(lambda x:link_dict[x] if x in link_dict else add_dict_link(x))\n","    #df['domains'] = df['domains'].map(lambda x: domain_dict[x] if x in domain_dict else add_dict_domain(x))\n","    #df['language'] = df['language'].map(lambda x: lang_dict[x] if x in lang_dict else add_dict_lang(x))\n","    #df['tweet_type'] = df['tweet_type'].map(lambda x: tweet_type_dict[x] if x in tweet_type_dict else add_dict_tweet_type(x))\n","    #df['a_user_id'] = df['a_user_id'].map(lambda x: user_dict[x] if x in user_dict else add_dict_user(x))\n","    #df['b_user_id'] = df['b_user_id'].map(lambda x: user_dict[x] if x in user_dict else add_dict_user(x))\n","    return df\n","\n","\n","# GPU pipeline\n","# Splits the entries in media by \\t and keeps only the first two values (if available)\n","def splitmedia(col):\n","    if col.shape[0] == 0:\n","        return(col)\n","    else:\n","        return(col.str.split('\\t', expand=True)[0].fillna('') + '_' + col.str.split('\\t', expand=True)[1].fillna(''))\n","\n","# Counts the number of token in a column (e.g. how many hashtags are in a tweet)  \n","def count_token(col,token):\n","    not_null = col.isnull()==0\n","    return ((col.str.count(token)+1)*not_null).fillna(0)\n","\n","# >> is an overloaded operator, it transforms columns in other columns applying functions to them\n","count_features = (\n","    nvt.ColumnGroup(['hashtags', 'domains', 'links', 'media']) >> (lambda col: count_token(col,'\\t')) >> nvt.ops.Rename(postfix = '_count_t')\n",")\n","\n","#split_media = nvt.ColumnGroup(['media']) >> (lambda col: splitmedia(col))\n","#split_media = nvt.ColumnGroup(['media']) >> (lambda col: splitmedia(col))\n","\n","# Dataset does not fit in memory, cannot use handy Categorify :(\n","multihot_filled = ['media', 'hashtags', 'domains', 'links'] >> nvt.ops.FillMissing()\n","\"\"\"cat_features = (\n","    split_media + multihot_filled + ['language', 'tweet_type', 'tweet_id', 'a_user_id', 'b_user_id'] >> \n","    nvt.ops.Categorify()\n",")\"\"\"\n","\n","LABEL_COLUMNS = ['reply', 'retweet', 'retweet_comment', 'like'] \n","label_name_feature = LABEL_COLUMNS >> nvt.ops.FillMissing()\n","labels = label_name_feature >> (lambda col: (col>0).astype('int8')) >> nvt.ops.Rename(postfix = '_engagement')\n","\n","weekday = (\n","    nvt.ColumnGroup(['timestamp']) >> \n","    (lambda col: cudf.to_datetime(col, unit='s').dt.weekday) >> \n","    nvt.ops.Rename(postfix = '_wd')\n",")\n","\n","datetime = nvt.ColumnGroup(['timestamp']) >> (lambda col: cudf.to_datetime(col.astype('int32'), unit='s'))\n","hour = datetime >> (lambda col: col.dt.hour) >> nvt.ops.Rename(postfix = '_hour')\n","minute = datetime >> (lambda col: col.dt.minute) >> nvt.ops.Rename(postfix = '_minute')\n","seconds = datetime >> (lambda col: col.dt.second) >> nvt.ops.Rename(postfix = '_second')\n","\n","output = ['language', 'tweet_type', 'tweet_id', 'a_user_id', 'b_user_id']+multihot_filled+count_features+label_name_feature+weekday+labels+hour+minute+seconds\n","\n","remaining_columns = [x for x in features if x not in (output.columns+['text_tokens'])]\n","\n","proc = nvt.Workflow(output+remaining_columns)\n","\n","\n","# For all features except tweet_id, use parts = [253] (one shot)\n","BASE_DIR = '/content/drive/MyDrive/RecSys2021/original_dataset'\n","parts = [54, 122, 189, 253]\n","#parts = [253]\n","first_run = True\n","old_s = 0\n","for pt in parts:\n","    print('Inizio passaggio da ' + str(old_s))\n","    print('Nuova BASE_DIR: ' + BASE_DIR)\n","    dir1 = 'preprocess_split_final'\n","    dir2 = 'preprocess_split_final_tmp_' + str(pt)\n","    ends = []\n","    i = old_s\n","    end_dict = pt\n","    end = parts[-1]\n","    while (i < end):\n","        j = i\n","        data_parts = []\n","        ends = []\n","        # Select one part\n","        for j in range(j,j+1):\n","            if (j<10):\n","                ends.append('0000' + str(j))\n","            elif (j<100):\n","                ends.append('000' + str(j))\n","            else:\n","                ends.append('00' + str(j))\n","        print(ends)\n","        for file in os.listdir(BASE_DIR):\n","            if file.endswith(tuple(ends)):\n","                data_parts.append(os.path.join(BASE_DIR, file))\n","\n","        # Preprocessing defined before only at first run on first feature\n","        if (old_s == 0 and first_run):\n","            trains_itrs = nvt.Dataset(data_parts, \n","                                      header=None, \n","                                      names=features, \n","                                      engine='csv', \n","                                      sep='\\x01', \n","                                      part_size='2GB')\n","\n","            time_preproc_start = time.time()\n","            proc.fit(trains_itrs)\n","            time_preproc = time.time()-time_preproc_start\n","            time_preproc\n","\n","            # We define the output datatypes for continuous columns to save memory. We can define the output datatypes as a dict and parse it to the to_parquet function\n","            dict_dtypes = {}\n","            for col in LABEL_COLUMNS + ['timestamp', 'a_follower_count', \n","                                        'a_following_count', 'a_account_creation',\n","                                        'b_follower_count', 'b_following_count', 'b_account_creation']:\n","                dict_dtypes[col] = np.uint32\n","\n","            time_preproc_start = time.time()\n","            proc.transform(trains_itrs).to_parquet(output_path='preprocess_out_1/', dtypes=dict_dtypes)\n","            time_preproc += time.time()-time_preproc_start\n","            time_preproc\n","\n","\n","        # Apply mappings with CPU\n","        if (old_s == 0 and first_run):\n","            df = dask.dataframe.read_parquet('preprocess_out_1/*.parquet')\n","        else:\n","            df = dask.dataframe.read_parquet(data_parts)\n","        if 'text_tokens' in list(df.columns):\n","            df = df.drop('text_tokens', axis=1)\n","\n","        df2 = df.map_partitions(simple_map, meta=df)\n","        df2.to_parquet('preprocess_out_2/')\n","\n","\n","        # Final encoding in parquet using GPU, saves more space\n","        df_tmp = cudf.read_parquet('preprocess_out_2/*.parquet')\n","        all_input_columns = df_tmp.columns\n","        #print(all_input_columns)\n","        del df_tmp\n","        gc.collect()\n","\n","        datetime = nvt.ColumnGroup(['timestamp']) >> (lambda col: cudf.to_datetime(col.astype('int32'), unit='s'))\n","        hour = datetime >> (lambda col: col.dt.hour) >> nvt.ops.Rename(postfix = '_hour')\n","        minute = datetime >> (lambda col: col.dt.minute) >> nvt.ops.Rename(postfix = '_minute')\n","        seconds = datetime >> (lambda col: col.dt.second) >> nvt.ops.Rename(postfix = '_second')\n","        output = hour+minute+seconds\n","        (output).graph\n","        remaining_columns = [x for x in all_input_columns if x not in (output.columns+['text_tokens'])]\n","\n","        dict_dtypes = {}\n","        for col in LABEL_COLUMNS + ['tweet_id', 'timestamp', 'a_follower_count', \n","                                'a_following_count', 'a_account_creation',\n","                                'b_follower_count', 'b_following_count', 'b_account_creation']:\n","            dict_dtypes[col] = np.uint32\n","        for col in ['reply_engagement', 'retweet_engagement', 'retweet_comment_engagement', 'like_engagement']:\n","            dict_dtypes[col] = np.uint8\n","\n","        # We initialize our NVTabular workflow and add the \"remaining\" columns to it\n","        proc2 = nvt.Workflow(output+remaining_columns)\n","\n","        train_dataset = nvt.Dataset(glob.glob('preprocess_out_2/*.parquet'), \n","                                    engine='parquet', \n","                                    part_size=\"2GB\")\n","\n","        proc2.fit(train_dataset)\n","        # If dictionary to big for ram, more passages are needed, so partially mapped files are saved in temporary folders\n","        if (j<end_dict):\n","            proc2.transform(train_dataset).to_parquet(output_path='/content/drive/MyDrive/RecSys2021/' + dir1, dtypes=dict_dtypes)\n","        else:\n","            proc2.transform(train_dataset).to_parquet(output_path='/content/drive/MyDrive/RecSys2021/' + dir2)\n","        \n","        if (os.path.isdir('preprocess_out_1/')):\n","            shutil.rmtree('preprocess_out_1/')\n","        if (os.path.isdir('preprocess_out_2/')): \n","            shutil.rmtree('preprocess_out_2/')\n","        \n","        gc.collect\n","        i = i + 1\n","\n","    print('Finito passaggio da ' + str(old_s))\n","    # Save dictionary\n","    with open('/content/drive/MyDrive/RecSys2021/dictionaries/tweet_id_dict_' + str(pt), 'w') as f:\n","        for chunk in json.JSONEncoder().iterencode(tweet_id_dict):\n","            f.write(chunk)\n","    # Reset dictionary\n","    old_size = tweet_id_dict['d_size']\n","    tweet_id_dict = {}\n","    tweet_id_dict['d_size'] = old_size + 1\n","\n","    # If needed, initalize next run\n","    old_s = end_dict\n","    BASE_DIR = '/content/drive/MyDrive/RecSys2021/' + dir2\n","    j = end_dict\n","    # Rename files in temporary folder sequentially\n","    if (end_dict != parts[-1]):\n","        for file in os.listdir(BASE_DIR):\n","            if file.endswith('.parquet'):\n","                if (j<10):\n","                    new_t = '0000' + str(j)\n","                    new = os.path.join(BASE_DIR, new_t)\n","                elif (j<100):\n","                    new_t = '000' + str(j)\n","                    new = os.path.join(BASE_DIR, new_t)\n","                else:\n","                    new_t = '00' + str(j)\n","                    new = os.path.join(BASE_DIR, new_t)\n","                old = os.path.join(BASE_DIR, file)\n","                os.rename(old, new)\n","                j = j + 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Inizio passaggio da 0\n","Nuova BASE_DIR: /content/drive/MyDrive/RecSys2021/original_dataset\n","['00000']\n","['00001']\n","['00002']\n","['00003']\n","['00004']\n","['00005']\n","['00006']\n","['00007']\n","['00008']\n","['00009']\n","['00010']\n","['00011']\n","['00012']\n","['00013']\n","['00014']\n","['00015']\n","['00016']\n","['00017']\n","['00018']\n","['00019']\n","['00020']\n","['00021']\n","['00022']\n","['00023']\n","['00024']\n","['00025']\n","['00026']\n","['00027']\n","['00028']\n","['00029']\n","['00030']\n","['00031']\n","['00032']\n","['00033']\n","['00034']\n","['00035']\n","['00036']\n","['00037']\n","['00038']\n","['00039']\n","['00040']\n","['00041']\n","['00042']\n","['00043']\n","['00044']\n","['00045']\n","['00046']\n","['00047']\n","['00048']\n","['00049']\n","['00050']\n","['00051']\n","['00052']\n","['00053']\n","['00054']\n","['00055']\n","['00056']\n","['00057']\n","['00058']\n","['00059']\n","['00060']\n","['00061']\n","['00062']\n","['00063']\n","['00064']\n","['00065']\n","['00066']\n","['00067']\n","['00068']\n","['00069']\n","['00070']\n","['00071']\n","['00072']\n","['00073']\n","['00074']\n","['00075']\n","['00076']\n","['00077']\n","['00078']\n","['00079']\n","['00080']\n","['00081']\n","['00082']\n","['00083']\n","['00084']\n","['00085']\n","['00086']\n","['00087']\n","['00088']\n","['00089']\n","['00090']\n","['00091']\n","['00092']\n","['00093']\n","['00094']\n","['00095']\n","['00096']\n","['00097']\n","['00098']\n","['00099']\n","['00100']\n","['00101']\n","['00102']\n","['00103']\n","['00104']\n","['00105']\n","['00106']\n","['00107']\n","['00108']\n","['00109']\n","['00110']\n","['00111']\n","['00112']\n","['00113']\n","['00114']\n","['00115']\n","['00116']\n","['00117']\n","['00118']\n","['00119']\n","['00120']\n","['00121']\n","['00122']\n","['00123']\n","['00124']\n","['00125']\n","['00126']\n","['00127']\n","['00128']\n","['00129']\n","['00130']\n","['00131']\n","['00132']\n","['00133']\n","['00134']\n","['00135']\n","['00136']\n","['00137']\n","['00138']\n","['00139']\n","['00140']\n","['00141']\n","['00142']\n","['00143']\n","['00144']\n","['00145']\n","['00146']\n","['00147']\n","['00148']\n","['00149']\n","['00150']\n","['00151']\n","['00152']\n","['00153']\n","['00154']\n","['00155']\n","['00156']\n","['00157']\n","['00158']\n","['00159']\n","['00160']\n","['00161']\n","['00162']\n","['00163']\n","['00164']\n","['00165']\n","['00166']\n","['00167']\n","['00168']\n","['00169']\n","['00170']\n","['00171']\n","['00172']\n","['00173']\n","['00174']\n","['00175']\n","['00176']\n","['00177']\n","['00178']\n","['00179']\n","['00180']\n","['00181']\n","['00182']\n","['00183']\n","['00184']\n","['00185']\n","['00186']\n","['00187']\n","['00188']\n","['00189']\n","['00190']\n","['00191']\n","['00192']\n","['00193']\n","['00194']\n","['00195']\n","['00196']\n","['00197']\n","['00198']\n","['00199']\n","['00200']\n","['00201']\n","['00202']\n","['00203']\n","['00204']\n","['00205']\n","['00206']\n","['00207']\n","['00208']\n","['00209']\n","['00210']\n","['00211']\n","['00212']\n","['00213']\n","['00214']\n","['00215']\n","['00216']\n","['00217']\n","['00218']\n","['00219']\n","['00220']\n","['00221']\n","['00222']\n","['00223']\n","['00224']\n","['00225']\n","['00226']\n","['00227']\n","['00228']\n","['00229']\n","['00230']\n","['00231']\n","['00232']\n","['00233']\n","['00234']\n","['00235']\n","['00236']\n","['00237']\n","['00238']\n","['00239']\n","['00240']\n","['00241']\n","['00242']\n","['00243']\n","['00244']\n","['00245']\n","['00246']\n","['00247']\n","['00248']\n","['00249']\n","['00250']\n","['00251']\n","['00252']\n","Finito passaggio da 0\n","Inizio passaggio da 54\n","Nuova BASE_DIR: /content/drive/MyDrive/RecSys2021/preprocess_split_final_tmp_54\n","['00054']\n","['00055']\n","['00056']\n","['00057']\n","['00058']\n","['00059']\n","['00060']\n","['00061']\n","['00062']\n","['00063']\n","['00064']\n","['00065']\n","['00066']\n","['00067']\n","['00068']\n","['00069']\n","['00070']\n","['00071']\n","['00072']\n","['00073']\n","['00074']\n","['00075']\n","['00076']\n","['00077']\n","['00078']\n","['00079']\n","['00080']\n","['00081']\n","['00082']\n","['00083']\n","['00084']\n","['00085']\n","['00086']\n","['00087']\n","['00088']\n","['00089']\n","['00090']\n","['00091']\n","['00092']\n","['00093']\n","['00094']\n","['00095']\n","['00096']\n","['00097']\n","['00098']\n","['00099']\n","['00100']\n","['00101']\n","['00102']\n","['00103']\n","['00104']\n","['00105']\n","['00106']\n","['00107']\n","['00108']\n","['00109']\n","['00110']\n","['00111']\n","['00112']\n","['00113']\n","['00114']\n","['00115']\n","['00116']\n","['00117']\n","['00118']\n","['00119']\n","['00120']\n","['00121']\n","['00122']\n","['00123']\n","['00124']\n","['00125']\n","['00126']\n","['00127']\n","['00128']\n","['00129']\n","['00130']\n","['00131']\n","['00132']\n","['00133']\n","['00134']\n","['00135']\n","['00136']\n","['00137']\n","['00138']\n","['00139']\n","['00140']\n","['00141']\n","['00142']\n","['00143']\n","['00144']\n","['00145']\n","['00146']\n","['00147']\n","['00148']\n","['00149']\n","['00150']\n","['00151']\n","['00152']\n","['00153']\n","['00154']\n","['00155']\n","['00156']\n","['00157']\n","['00158']\n","['00159']\n","['00160']\n","['00161']\n","['00162']\n","['00163']\n","['00164']\n","['00165']\n","['00166']\n","['00167']\n","['00168']\n","['00169']\n","['00170']\n","['00171']\n","['00172']\n","['00173']\n","['00174']\n","['00175']\n","['00176']\n","['00177']\n","['00178']\n","['00179']\n","['00180']\n","['00181']\n","['00182']\n","['00183']\n","['00184']\n","['00185']\n","['00186']\n","['00187']\n","['00188']\n","['00189']\n","['00190']\n","['00191']\n","['00192']\n","['00193']\n","['00194']\n","['00195']\n","['00196']\n","['00197']\n","['00198']\n","['00199']\n","['00200']\n","['00201']\n","['00202']\n","['00203']\n","['00204']\n","['00205']\n","['00206']\n","['00207']\n","['00208']\n","['00209']\n","['00210']\n","['00211']\n","['00212']\n","['00213']\n","['00214']\n","['00215']\n","['00216']\n","['00217']\n","['00218']\n","['00219']\n","['00220']\n","['00221']\n","['00222']\n","['00223']\n","['00224']\n","['00225']\n","['00226']\n","['00227']\n","['00228']\n","['00229']\n","['00230']\n","['00231']\n","['00232']\n","['00233']\n","['00234']\n","['00235']\n","['00236']\n","['00237']\n","['00238']\n","['00239']\n","['00240']\n","['00241']\n","['00242']\n","['00243']\n","['00244']\n","['00245']\n","['00246']\n","['00247']\n","['00248']\n","['00249']\n","['00250']\n","['00251']\n","['00252']\n","Finito passaggio da 54\n","Inizio passaggio da 122\n","Nuova BASE_DIR: /content/drive/MyDrive/RecSys2021/preprocess_split_final_tmp_122\n","['00122']\n","['00123']\n","['00124']\n","['00125']\n","['00126']\n","['00127']\n","['00128']\n","['00129']\n","['00130']\n","['00131']\n","['00132']\n","['00133']\n","['00134']\n","['00135']\n","['00136']\n","['00137']\n","['00138']\n","['00139']\n","['00140']\n","['00141']\n","['00142']\n","['00143']\n","['00144']\n","['00145']\n","['00146']\n","['00147']\n","['00148']\n","['00149']\n","['00150']\n","['00151']\n","['00152']\n","['00153']\n","['00154']\n","['00155']\n","['00156']\n","['00157']\n","['00158']\n","['00159']\n","['00160']\n","['00161']\n","['00162']\n","['00163']\n","['00164']\n","['00165']\n","['00166']\n","['00167']\n","['00168']\n","['00169']\n","['00170']\n","['00171']\n","['00172']\n","['00173']\n","['00174']\n","['00175']\n","['00176']\n","['00177']\n","['00178']\n","['00179']\n","['00180']\n","['00181']\n","['00182']\n","['00183']\n","['00184']\n","['00185']\n","['00186']\n","['00187']\n","['00188']\n","['00189']\n","['00190']\n","['00191']\n","['00192']\n","['00193']\n","['00194']\n","['00195']\n","['00196']\n","['00197']\n","['00198']\n","['00199']\n","['00200']\n","['00201']\n","['00202']\n","['00203']\n","['00204']\n","['00205']\n","['00206']\n","['00207']\n","['00208']\n","['00209']\n","['00210']\n","['00211']\n","['00212']\n","['00213']\n","['00214']\n","['00215']\n","['00216']\n","['00217']\n","['00218']\n","['00219']\n","['00220']\n","['00221']\n","['00222']\n","['00223']\n","['00224']\n","['00225']\n","['00226']\n","['00227']\n","['00228']\n","['00229']\n","['00230']\n","['00231']\n","['00232']\n","['00233']\n","['00234']\n","['00235']\n","['00236']\n","['00237']\n","['00238']\n","['00239']\n","['00240']\n","['00241']\n","['00242']\n","['00243']\n","['00244']\n","['00245']\n","['00246']\n","['00247']\n","['00248']\n","['00249']\n","['00250']\n","['00251']\n","['00252']\n","Finito passaggio da 122\n","Inizio passaggio da 189\n","Nuova BASE_DIR: /content/drive/MyDrive/RecSys2021/preprocess_split_final_tmp_189\n","['00189']\n","['00190']\n","['00191']\n","['00192']\n","['00193']\n","['00194']\n","['00195']\n","['00196']\n","['00197']\n","['00198']\n","['00199']\n","['00200']\n","['00201']\n","['00202']\n","['00203']\n","['00204']\n","['00205']\n","['00206']\n","['00207']\n","['00208']\n","['00209']\n","['00210']\n","['00211']\n","['00212']\n","['00213']\n","['00214']\n","['00215']\n","['00216']\n","['00217']\n","['00218']\n","['00219']\n","['00220']\n","['00221']\n","['00222']\n","['00223']\n","['00224']\n","['00225']\n","['00226']\n","['00227']\n","['00228']\n","['00229']\n","['00230']\n","['00231']\n","['00232']\n","['00233']\n","['00234']\n","['00235']\n","['00236']\n","['00237']\n","['00238']\n","['00239']\n","['00240']\n","['00241']\n","['00242']\n","['00243']\n","['00244']\n","['00245']\n","['00246']\n","['00247']\n","['00248']\n","['00249']\n","['00250']\n","['00251']\n","['00252']\n","Finito passaggio da 189\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AWcgzcuvKQcE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618763032789,"user_tz":-120,"elapsed":961,"user":{"displayName":"Giacomo Lodigiani","photoUrl":"","userId":"15410861073597855322"}},"outputId":"126104e7-ff35-4ebb-8f44-a2f6e0823be2"},"source":["# Check \n","import pandas as pd\n","df = dask.dataframe.read_parquet('/content/drive/MyDrive/RecSys2021/preprocess_split_final/0.735b0fb3001b48018cb6c91437893999.parquet')\n","pd.set_option('display.max_rows', 1000)\n","#df.head(1000)\n","print(tweet_id_dict['d_size'])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["301421584\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A76gLuDJgva4"},"source":["# Get real size of objects\n","def get_size(obj, seen=None):\n","    \"\"\"Recursively finds size of objects\"\"\"\n","    size = sys.getsizeof(obj)\n","    if seen is None:\n","        seen = set()\n","    obj_id = id(obj)\n","    if obj_id in seen:\n","        return 0\n","    # Important mark as seen *before* entering recursion to gracefully handle\n","    # self-referential objects\n","    seen.add(obj_id)\n","    if isinstance(obj, dict):\n","        size += sum([get_size(v, seen) for v in obj.values()])\n","        size += sum([get_size(k, seen) for k in obj.keys()])\n","    elif hasattr(obj, '__dict__'):\n","        size += get_size(obj.__dict__, seen)\n","    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n","        size += sum([get_size(i, seen) for i in obj])\n","    return size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dWPgCHUEG7tp"},"source":["# To avoid OOM, write dictionary to file in parts\n","import json\n","\n","with open('/content/drive/MyDrive/RecSys2021/dictionaries/tweet_id_dict_1', 'w') as f:\n","    for chunk in json.JSONEncoder().iterencode(tweet_id_dict):\n","        f.write(chunk)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XRBvlpibWNc9","executionInfo":{"status":"ok","timestamp":1618618120601,"user_tz":-120,"elapsed":3294,"user":{"displayName":"Giacomo Lodigiani","photoUrl":"","userId":"15410861073597855322"}},"outputId":"bea78e45-4e04-442a-e729-f0ee19223874"},"source":["# To avoid OOM, read dictionary from file in parts\n","!pip install ijson\n","\n","import ijson\n","\n","def parse_json(json_filename, dic):\n","    with open(json_filename, 'rb') as input_file:\n","        # load json iteratively\n","        parser = ijson.parse(input_file)\n","        for prefix, event, value in parser:\n","            if (prefix):\n","                dic[prefix] = value\n","        return dic\n","\n","tweet_id_dict = {}\n","tweet_id_dict = parse_json('/content/drive/MyDrive/RecSys2021/dictionaries/tweet_id_dict_1', tweet_id_dict)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: ijson in /usr/local/lib/python3.7/site-packages (3.1.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_92Uzlqe9BV_"},"source":["# Rename files in folder\n","import os\n","\n","j = 0\n","for file in os.listdir(BASE_DIR + '_preprocess_split_final_pt2/'):\n","    if file.endswith('.parquet'):\n","        if (j<10):\n","            new_t = '0000' + str(j) + '.parquet'\n","            new = os.path.join(BASE_DIR_2, new_t)\n","        elif (j<100):\n","            new_t = '000' + str(j) + '.parquet'\n","            new = os.path.join(BASE_DIR_2, new_t)\n","        else:\n","            new_t = '00' + str(j) + '.parquet'\n","            new = os.path.join(BASE_DIR_2, new_t)\n","        old = os.path.join(BASE_DIR_2, file)\n","        os.rename(old, new)\n","        j = j + 1"],"execution_count":null,"outputs":[]}]}