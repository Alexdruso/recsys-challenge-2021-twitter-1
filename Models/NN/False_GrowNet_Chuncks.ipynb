{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GrowNet Chuncks.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Vctg7k-kn5R7",
        "67w3_D5WbwAP",
        "DAXT2VHrouEs",
        "xzd7VhKnouE_",
        "c_v7zmjrouFD",
        "b0Ff_BXJ54TY",
        "r1zFs-Hm54UK",
        "4XpMDjFh54UR"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Wrq1qypev6"
      },
      "source": [
        "%%capture\n",
        "!apt-get install  aria2\n",
        "!apt-get install lzop\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIkd0HIYXT59"
      },
      "source": [
        "%%capture\n",
        "import os, json, zipfile, shutil, platform, time\n",
        "\n",
        "import scipy.sparse as sps\n",
        "from pandas import DataFrame\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import multiprocessing\n",
        "import multiprocessing.pool\n",
        "import time\n",
        "\n",
        "from random import randint\n",
        "from tensorflow import keras\n",
        "targetCol=\"like\"\n",
        "targets=[\"like\",\t\"retweet\",\t\"retweetCom\",\t\"reply\",\t\"interaction\"]\n",
        "droppable=[t for t in targets if t!=targetCol]\n",
        "import gc\n",
        "import numpy as np\n",
        "np.random.seed(1234)\n",
        "gc.collect()\n",
        "\n",
        "#Disable copy warning (hopefully are false positive here)\n",
        "pd.options.mode.chained_assignment = None"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBnrMTNPHZw8"
      },
      "source": [
        "all_features = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\\\n",
        "                \"tweet_type\",\"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\\\n",
        "               \"engaged_with_user_following_count\", \"engaged_with_user_is_verified\", \"engaged_with_user_account_creation\",\\\n",
        "               \"engaging_user_id\", \"engaging_user_follower_count\", \"engaging_user_following_count\", \"engaging_user_is_verified\",\\\n",
        "               \"engaging_user_account_creation\", \"engagee_follows_engager\",\"reply_timestamp\", \"retweet_timestamp\", \"retweet_with_comment_timestamp\", \"like_timestamp\"]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vctg7k-kn5R7"
      },
      "source": [
        "#Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAlpu6Lrn6QI"
      },
      "source": [
        "from sklearn.metrics import average_precision_score, log_loss\n",
        "def calculate_ctr(gt):\n",
        "    positive = len([x for x in gt if x == 1])\n",
        "    ctr = positive/float(len(gt))\n",
        "    return ctr\n",
        "\n",
        "def compute_rce(pred, gt):\n",
        "    cross_entropy = log_loss(gt, pred-1e-7)\n",
        "    data_ctr = calculate_ctr(gt)\n",
        "    strawman_cross_entropy = log_loss(gt, [data_ctr for _ in range(len(gt))])\n",
        "    return (1.0 - cross_entropy/(strawman_cross_entropy+1e-7))*100.0\n",
        "\n",
        "def calculate_metrics(prediction,targetTest):\n",
        "    rce = compute_rce(prediction,targetTest)\n",
        "    average_precision = average_precision_score(targetTest,prediction)\n",
        "    print(f\"rce={rce}\")\n",
        "    print(f\"AP={average_precision}\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67w3_D5WbwAP"
      },
      "source": [
        "# Preprocessing function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRG3snsqbwAR"
      },
      "source": [
        "###Special Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjT6WteXbwAS"
      },
      "source": [
        "HTTPS_token = '14120' \n",
        "RT_token = '56898'\n",
        "at_token = '137'\n",
        "hashtag_token = '108'\n",
        "gt_token = '135'\n",
        "lt_token = '133'\n",
        "amp_token = '111'\n",
        "question_token = '136'\n",
        "esclamation_token = '106'\n",
        "period_token = '119'\n",
        "two_periods_token = '131'\n",
        "coma_token = '117'\n",
        "dollar_token = '109'\n",
        "period_coma_token = '132'\n",
        "parenthesis_open_token = '113'\n",
        "parenthesis_closed_token = '114'\n",
        "star_token = '115'\n",
        "slash_token = '120'\n",
        "line_token = '118'\n",
        "underscore_token = '168'\n",
        "tilde_token = '198'\n",
        "virgolette_token = '107'\n",
        "square_parenthesis_open_token = '164'\n",
        "square_parenthesis_closed_token = '166'\n",
        "unk_token = '100'\n",
        "others_tokens = ['11733', '12022']\n",
        "twitter_internal_link=\"188\" #\"t\":188,#twitter links\n",
        "co_domain_token= \"11170\" #\"co\":11170\n",
        "eight=\"129\" #\"8\":129 ???? forse utile\n",
        "\n",
        "special_tokens_list = [\n",
        "    at_token,\n",
        "    hashtag_token,\n",
        "    gt_token,\n",
        "    lt_token,\n",
        "    amp_token,\n",
        "    question_token,\n",
        "    esclamation_token,\n",
        "    period_token,\n",
        "    coma_token,\n",
        "    dollar_token,\n",
        "    period_coma_token,\n",
        "    two_periods_token,\n",
        "    parenthesis_open_token,\n",
        "    parenthesis_closed_token,\n",
        "    star_token,\n",
        "    slash_token,\n",
        "    line_token,\n",
        "    underscore_token,\n",
        "    tilde_token,\n",
        "    virgolette_token,\n",
        "    square_parenthesis_open_token,\n",
        "    square_parenthesis_closed_token,\n",
        "    unk_token,\n",
        "    twitter_internal_link, \n",
        "    co_domain_token,\n",
        "    eight, \n",
        "]\n",
        "\n",
        "jimin=\"27128\"\n",
        "V=\"159\"\n",
        "BTS=\"BTS\"\n",
        "btsTokenized=\"70447\\t10731\\t\"\n",
        "BE=\"46291\" #album name\n",
        "bts=[\n",
        "    jimin,\n",
        "    V,\n",
        "    BE,\n",
        "    BTS\n",
        "]\n",
        "\n",
        "EA=\"38478\"\n",
        "SPORTS=\"15506\"\n",
        "\n",
        "sport=[\n",
        "    EA,\n",
        "    SPORTS,\n",
        "]\n",
        "\n",
        "quarterback=\"154\"\n",
        "ncaa=\"23864\"\n",
        "FOOTBALL=\"12499\"\n",
        "nfl=\"20179\"\n",
        "Denard_Robinson=[11274\t,10235]\n",
        "football=[\n",
        "        *sport,\n",
        "        quarterback,\n",
        "        ncaa,\n",
        "        FOOTBALL,\n",
        "        nfl\n",
        "]\n",
        "def countList(list_specific):\n",
        "    def count_f(row):\n",
        "        row=row.split(\"\\t\")\n",
        "        count=0\n",
        "        for el in row:\n",
        "            if el in list_specific:\n",
        "                count+=1\n",
        "        return count\n",
        "    return count_f\n",
        "\n",
        "countSpecial=countList(special_tokens_list)\n",
        "\n",
        "countBTS=countList(bts)\n",
        "\n",
        "countSports=countList(sport)\n",
        "\n",
        "countFootball=countList(football)\n",
        "\n",
        "\n",
        "def countSpecificWord(word):\n",
        "    def count_f(row):\n",
        "        row=row.split(\"\\t\")\n",
        "        return row.count(word)\n",
        "    return count_f\n",
        "\n",
        "def countWordSet(words):\n",
        "    def count_f(row):\n",
        "        row=row.split(\"\\t\")\n",
        "        count=0\n",
        "        for w in words:\n",
        "            count+=row.count(w)\n",
        "        return w\n",
        "    return count_f\n",
        "\n",
        "def checkBalanced(a,b):\n",
        "    def count_f(row):\n",
        "        row=row.split(\"\\t\")\n",
        "        counta=row.count(a)\n",
        "        countb=row.count(b)\n",
        "        return 1 if counta==countb else 0\n",
        "    return count_f\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOF2DxSUbwAU"
      },
      "source": [
        "def removeRT(row):\n",
        "    if row.find(\"101\\t56898\")!=-1:\n",
        "        idx=row.find(\"131\")\n",
        "        return row[:3]+row[idx+3:]\n",
        "    return row"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8L7S5bjkCel"
      },
      "source": [
        "###Hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87E9RLLpZQZs"
      },
      "source": [
        "origin=\"drive/MyDrive/twitter/\"\n",
        "files=[\"ErdoganHashtags.bz2\",\"BigBrotherHashtags.bz2\",\n",
        "       \"BTSHashtags.bz2\",\"NCTHashtags.bz2\",\"GOT7Hashtags.bz2\",\"ThaiHashtags.bz2\",\n",
        "       \"BirmaniaHashtags.bz2\",\"CovidHashtags.bz2\",\"ImpactHashtags.bz2\",\"PeingHashtags.bz2\"]\n",
        "res=[]\n",
        "import json\n",
        "\n",
        "for file in files:\n",
        "    os.system(f\"cp {origin+file} {file}\")\n",
        "    os.system(f\"bunzip2 {file}\")\n",
        "    os.system(f\"mv {file[:-4]} {file[:-4]}.json\")\n",
        "    with open(f'{file[:-4]}.json') as f:\n",
        "        data = json.load(f)\n",
        "        res.append(set(data.keys()))\n",
        "        del data\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "countHashtagFunctions=[]\n",
        "for i in range(len(files)):\n",
        "    countHashtagFunctions.append([f\"count_{i}\",countList(res[i])])\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxzJTJLMbwAV"
      },
      "source": [
        "###actual code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xock9leQbwAW"
      },
      "source": [
        "def countMedia(arr,mediaType):\n",
        "        c=0\n",
        "        for m in arr:\n",
        "            if m==mediaType:\n",
        "                c+=1\n",
        "        return c"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdzv_iUebwAW"
      },
      "source": [
        "import re\n",
        "summary={}\n",
        "def computeTrainTest(verbose=False,userSpecific=False):\n",
        "    df=pd.read_csv(\"train.csv\",header=None,sep='\\x01')\n",
        "    df.columns=all_features\n",
        "\n",
        "    df[\"like\"]=df[\"like_timestamp\"].apply(lambda row: 1 if row>0 else 0).astype(\"uint8\")\n",
        "    df.drop(\"like_timestamp\",axis=1,inplace=True)\n",
        "\n",
        "    df[\"retweet\"]=df[\"retweet_timestamp\"].apply(lambda row: 1 if row>0 else 0).astype(\"uint8\")\n",
        "    df.drop(\"retweet_timestamp\",axis=1,inplace=True)\n",
        "\n",
        "    df[\"retweetCom\"]=df[\"retweet_with_comment_timestamp\"].apply(lambda row: 1 if row>0 else 0).astype(\"uint8\")\n",
        "    df.drop(\"retweet_with_comment_timestamp\",axis=1,inplace=True)\n",
        "\n",
        "    df[\"reply\"]=df[\"reply_timestamp\"].apply(lambda row: 1 if row>0 else 0).astype(\"uint8\")\n",
        "    df.drop(\"reply_timestamp\",axis=1,inplace=True)\n",
        "\n",
        "    df[\"interaction\"]=df[\"reply\"]+df[\"retweetCom\"]+df[\"retweet\"]+df[\"like\"]\n",
        "\n",
        "    df[\"interaction\"][df[\"interaction\"]>0]=1\n",
        "\n",
        "\n",
        "    df.fillna(value={\"present_media\":\"\"},inplace=True)\n",
        "    df[\"photos\"]=df[\"present_media\"].apply(lambda row: countMedia((row.split(\"\\t\")),\"Photo\")).astype(\"uint8\")\n",
        "    df[\"gifs\"]=df[\"present_media\"].apply(lambda row: countMedia((row.split(\"\\t\")),\"GIF\")).astype(\"uint8\")\n",
        "    df[\"videos\"]=df[\"present_media\"].apply(lambda row: countMedia((row.split(\"\\t\")),\"Video\")).astype(\"uint8\")\n",
        "\n",
        "    #df[\"NoText\"]=df[\"text_tokens\"].apply(lambda row:  False if len(row.split(\"\\t\"))>2 else True).astype(\"uint16\")\n",
        "\n",
        "    df[\"distinct_text_tokens\"]=df[\"text_tokens\"].apply(lambda row: len(set(row.split(\"\\t\")))-2 if row!=\"\" else 0).astype(\"uint16\")\n",
        "\n",
        "    df[\"special_characters\"]=df[\"text_tokens\"].apply(countSpecial).astype(\"uint16\")\n",
        "\n",
        "    df[\"text_tokens\"]=df[\"text_tokens\"].apply(lambda row:removeRT(row))\n",
        "\n",
        "    #remove links\n",
        "    df[\"text_tokens\"]=df[\"text_tokens\"].apply(lambda row: re.sub(\"14120\\t131\\t120\\t120\\t188\\t119\\t11170\\t120\\t\\d+\\t\\d+\\t\\d+\\t\\d+\\t\\d+\\t\\d+\\t\\d+\\t\\d+\\t\",\"\",row) )\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    df[\"text_tokens\"]=df[\"text_tokens\"].apply(lambda row: re.sub(\"70447\\t10731\\t\",\"BTS\",row))\n",
        "\n",
        "    df[\"isReplying\"]=df[\"text_tokens\"].apply(lambda row: row.startswith(\"101\\t137\"))\n",
        "\n",
        "    df[\"unknown\"]=df[\"text_tokens\"].apply(countSpecificWord(unk_token)).astype(\"uint16\")\n",
        "    \n",
        "    #df[\"sentence\"]=df[\"text_tokens\"].apply(countSpecificWord(period_token)).astype(\"uint16\")\n",
        "\n",
        "    df[\"esclamations\"]=df[\"text_tokens\"].apply(countSpecificWord(esclamation_token)).astype(\"uint16\")\n",
        "\n",
        "    df[\"questions\"]=df[\"text_tokens\"].apply(countSpecificWord(question_token)).astype(\"uint16\")\n",
        "\n",
        "    df[\"mentions\"]=df[\"text_tokens\"].apply(countSpecificWord(at_token)).astype(\"uint16\")\n",
        "\n",
        "    df[\"twitter_Specific\"]=df[\"text_tokens\"].apply(countWordSet([RT_token,at_token,hashtag_token])).astype(\"uint16\")\n",
        "\n",
        "    #RT token at the beginning is removed so remaining RT tokens should indicate a request for retweet\n",
        "    df[\"asking_retwet\"]=df[\"text_tokens\"].apply(countWordSet([RT_token])).astype(\"uint16\")\n",
        "\n",
        "    df[\"ask_reply\"]=df[\"text_tokens\"].apply(lambda row: row.count(\"76456\\t10454\"))\n",
        "\n",
        "    #Noisy the word like has other meanings\n",
        "    df[\"ask_like\"]=df[\"text_tokens\"].apply(lambda row: row.count(\"11850\"))\n",
        "\n",
        "\n",
        "    df[\"subsentence\"]=df[\"text_tokens\"].apply(countSpecificWord(coma_token)).astype(\"uint16\")\n",
        "\n",
        "    df[\"subsentence_semantic_separation\"]=df[\"text_tokens\"].apply(countSpecificWord(period_coma_token)).astype(\"uint16\")\n",
        "\n",
        "    #df[\"BTS\"]=df[\"text_tokens\"].apply(countBTS).astype(\"uint16\")\n",
        "\n",
        "    #df[\"Sports\"]=df[\"text_tokens\"].apply(countSports).astype(\"uint16\")\n",
        "\n",
        "    #df[\"Football\"]=df[\"text_tokens\"].apply(countFootball).astype(\"uint16\")\n",
        "\n",
        "    #df[\"proportion\"]=df.apply(lambda row: row[\"distinct_text_tokens\"]/max(1,row[\"special_characters\"]))\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    #df[\"balanced_angular\"]=df[\"text_tokens\"].apply(checkBalanced(gt_token,lt_token)).astype(\"uint16\")\n",
        "\n",
        "    #df[\"balanced_square\"]=df[\"text_tokens\"].apply(checkBalanced(square_parenthesis_open_token,square_parenthesis_closed_token)).astype(\"uint16\")\n",
        "\n",
        "    #df[\"balanced_round\"]=df[\"text_tokens\"].apply(checkBalanced(parenthesis_open_token,parenthesis_closed_token,)).astype(\"uint16\")\n",
        "\n",
        "    df[\"parenthesis\"]=df[\"text_tokens\"].apply(countWordSet([parenthesis_open_token,parenthesis_closed_token,gt_token,lt_token,square_parenthesis_open_token,square_parenthesis_closed_token])).astype(\"uint16\")\n",
        "    \n",
        "    \n",
        "\n",
        "    df[\"text_tokens\"]=df[\"text_tokens\"].apply(lambda row: len(row.split(\"\\t\")) if row!=\"\" else 0).astype(\"uint16\")\n",
        "    df.fillna(value={\"hashtags\":\"\"},inplace=True)\n",
        "    \n",
        "    \n",
        "\n",
        "    df[\"present_media\"]=df[\"present_media\"].apply(lambda row: len(row.split(\"\\t\")) if row!=\"\" else 0).astype(\"uint8\")\n",
        "    df.fillna(value={\"present_links\":\"\"},inplace=True)\n",
        "    df[\"present_links\"]=df[\"present_links\"].apply(lambda row: len(row.split(\"\\t\")) if row!=\"\" else 0).astype(\"uint8\")\n",
        "    df.fillna(value={\"present_domains\":\"\"},inplace=True)\n",
        "    df[\"present_domains\"]=df[\"present_domains\"].apply(lambda row: len(row.split(\"\\t\")) if row!=\"\" else 0).astype(\"uint8\")\n",
        "    \n",
        "    df=pd.get_dummies(df,columns=[\"tweet_type\"])\n",
        "\n",
        "    gc.collect()\n",
        "    df.drop(\"tweet_id\",axis=1,inplace=True)\n",
        "    df.drop(\"engaging_user_account_creation\",axis=1,inplace=True)\n",
        "    df.drop(\"engaged_with_user_account_creation\",axis=1,inplace=True)\n",
        "    df.drop(\"tweet_timestamp\",axis=1,inplace=True)\n",
        "\n",
        "    df.drop(\"engaged_with_user_id\",axis=1,inplace=True)\n",
        "    \n",
        "    df.drop(\"language\",axis=1,inplace=True)\n",
        "    \n",
        "    \n",
        "    #TODO Normalization\n",
        "    msk = np.random.rand(len(df)) < 0.8\n",
        "    train=df[msk]\n",
        "    test=df[~msk]\n",
        "    \n",
        "    hashtagCols=[]\n",
        "    print(\"here\")\n",
        "    for f in countHashtagFunctions:\n",
        "        hashtagCols.append(f[0])\n",
        "        train[f[0]]=train[\"hashtags\"].apply(f[1]).astype(\"uint8\")\n",
        "        test[f[0]]=test[\"hashtags\"].apply(f[1]).astype(\"uint8\")\n",
        "        \n",
        "    if userSpecific:\n",
        "        tempTable=train[train[\"interaction\"]>0].copy()\n",
        "\n",
        "        tempTable=tempTable[[*hashtagCols,\"engaging_user_id\"]]\n",
        "\n",
        "        tempTable=tempTable.groupby([\"engaging_user_id\"]).sum()\n",
        "\n",
        "        trainTable=tempTable.copy()\n",
        "        testTable=tempTable.copy()\n",
        "        for c in hashtagCols:\n",
        "            trainTable[c]=trainTable[c].apply(lambda row: 0 if row<2 else 1)\n",
        "            testTable[c]=testTable[c].apply(lambda row: 0 if row<1 else 1)\n",
        "        if verbose:\n",
        "            print(trainTable[:20])\n",
        "        train=train.merge(trainTable,how=\"left\",on=\"engaging_user_id\")\n",
        "        \n",
        "        test=test.merge(testTable,how=\"left\",on=\"engaging_user_id\")\n",
        "        \n",
        "\n",
        "    train.fillna(0,inplace=True)\n",
        "    test.fillna(0,inplace=True)\n",
        "    train.drop(\"engaging_user_id\",axis=1,inplace=True)\n",
        "    test.drop(\"engaging_user_id\",axis=1,inplace=True)\n",
        "\n",
        "    train[\"hashtags\"]=train[\"hashtags\"].apply(lambda row: len(row.split(\"\\t\")) if row!=\"\" else 0).astype(\"uint8\")\n",
        "    test[\"hashtags\"]=test[\"hashtags\"].apply(lambda row: len(row.split(\"\\t\")) if row!=\"\" else 0).astype(\"uint8\")\n",
        "\n",
        "    for col in train.columns:\n",
        "        if col  in droppable:\n",
        "            train.drop(col,axis=1,inplace=True)\n",
        "            test.drop(col,axis=1,inplace=True)\n",
        "        elif col in [\"engaged_with_user_id\",\"engaging_user_id\",\"language\"]:\n",
        "            train.drop(col,axis=1,inplace=True)\n",
        "            test.drop(col,axis=1,inplace=True)\n",
        "        else:\n",
        "            train[col]=train[col].astype(\"float32\")\n",
        "            test[col]=test[col].astype(\"float32\")\n",
        "    for col in train.columns:\n",
        "        if col!=targetCol:\n",
        "            if col not in summary:\n",
        "                summary[col]={\"mean\":np.mean(train[col].values),\"std\":np.std(train[col])}\n",
        "            if summary[col][\"std\"]==0:\n",
        "                train[col]=0\n",
        "                test[col]=0\n",
        "            else:\n",
        "                train[col]=(train[col]-summary[col][\"mean\"])/summary[col][\"std\"]\n",
        "                test[col]=(test[col]-summary[col][\"mean\"])/summary[col][\"std\"]\n",
        "\n",
        "\n",
        "    target=train[targetCol]\n",
        "    target=target.astype(\"float32\")\n",
        "    target=target.values\n",
        "    targetTest=test[targetCol]\n",
        "    targetTest=targetTest.astype(\"float32\")\n",
        "    targetTest=targetTest.values\n",
        "    train.drop(targetCol,axis=1,inplace=True)\n",
        "    test.drop(targetCol,axis=1,inplace=True)\n",
        "    input=train.values\n",
        "    inputTest=test.values\n",
        "    del df\n",
        "    del train\n",
        "    del test\n",
        "    return (input,target,inputTest,targetTest)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVmIE-nsvFO2"
      },
      "source": [
        "#Code for model adding features from old trained networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuIdfPoZmEj"
      },
      "source": [
        "##NN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auavUQ8_f-l7"
      },
      "source": [
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from math import ceil\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense,Activation,Dropout\n",
        "from keras.optimizers import RMSprop\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slc-tix2f9MA"
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "def create_weighted_binary_crossentropy(zero_weight, one_weight):\n",
        "    def weighted_binary_crossentropy(y_true, y_pred):\n",
        "        b_ce = K.binary_crossentropy(y_true, y_pred)\n",
        "        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n",
        "        weighted_b_ce = weight_vector * b_ce\n",
        "        return K.mean(weighted_b_ce)\n",
        "    return weighted_binary_crossentropy"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPbDlM1PgBBF"
      },
      "source": [
        "def buildModel(inputSize):\n",
        "    model = Sequential()\n",
        "    size=128\n",
        "    reduction_factor=2\n",
        "    shape=(inputSize,)\n",
        "    depth=2\n",
        "    dropout=0.4\n",
        "    kernel_resularizer_norm=1e-5\n",
        "    for i in range(depth):\n",
        "                model.add(Dense(size,  input_shape=shape,kernel_regularizer=regularizers.l2(kernel_resularizer_norm)))\n",
        "                model.add(Activation(\"relu\"))\n",
        "                model.add(Dropout(dropout))\n",
        "                size=size//reduction_factor\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(Dense(32,kernel_regularizer=regularizers.l2(kernel_resularizer_norm)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1,kernel_regularizer=regularizers.l2(1e-4)))\n",
        "    model.add(Activation(\"sigmoid\"))\n",
        "    model2=keras.Model(inputs=[model.inputs],outputs=model.layers[-3].output)\n",
        "\n",
        "    return (model,model2)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azV5dTs_ZnX2"
      },
      "source": [
        "import copy\n",
        "def computeLast(arr,last):\n",
        "    return last.predict(arr)\n",
        "\n",
        "from tqdm import tqdm\n",
        "def trainModel(train,trainTarget,test,testTarget,last=None):\n",
        "    if last!=None:\n",
        "        originTrain=copy.deepcopy(train)\n",
        "        originTest=copy.deepcopy(test)\n",
        "        for m in tqdm(last, position=0, leave=True):\n",
        "            temp=computeLast(train,m)\n",
        "            train=np.hstack((originTrain,temp))\n",
        "            temp=computeLast(test,m)\n",
        "            test=np.hstack((originTest,temp))\n",
        "            gc.collect()\n",
        "        del temp\n",
        "    count=len(train[0])\n",
        "    print(f\"features: {count}\")\n",
        "    model,model2=buildModel(count)\n",
        "    BS=64\n",
        "    loss=create_weighted_binary_crossentropy(1,1.1)\n",
        "    model.compile(loss=loss, metrics=[tf.keras.metrics.AUC(curve=\"PR\",name=\"PRAUC\"),\"accuracy\"],optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003))\n",
        "    model.fit(train,trainTarget,epochs=1,batch_size=BS)\n",
        "    prediction=model.predict(test)\n",
        "    calculate_metrics(prediction,testTarget)\n",
        "    return (model,model2)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBpXMIEmvK45"
      },
      "source": [
        "##Chunk wise download and work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paaBAbtPvNlv"
      },
      "source": [
        "import os \n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "gc.collect()\n",
        "modelComp=[]\n",
        "modelParz=[]\n",
        "index=0\n",
        "with open(\"training_urls.txt\") as doc:\n",
        "    index=0\n",
        "    for line in doc:#tqdm(doc, position=0, leave=True):\n",
        "        #skip lzo.index files (odd row number)\n",
        "        if index%2==1:\n",
        "            index+=1\n",
        "            continue\n",
        "        os.system(f\"aria2c '{line}'\")\n",
        "        partName=str(index//2)\n",
        "        print(partName)\n",
        "        partName=\"0\"*(5-len(partName))+partName\n",
        "        \n",
        "        os.system(f'lzop -x part-{partName}.lzo')\n",
        "        os.system(f'mv part-{partName} train.csv')\n",
        "        os.system(f\"rm part-{partName}.lzo\")\n",
        "        train,trainTarget,test,testTarget=computeTrainTest()\n",
        "        if len(modelComp)==0:\n",
        "            model1,model2=trainModel(train,trainTarget,test,testTarget)\n",
        "        else:\n",
        "            model1,model2=trainModel(train,trainTarget,test,testTarget,last=modelComp)\n",
        "        modelComp.append(model1)\n",
        "        modelParz.append(model2)\n",
        "        os.system(\"rm train.csv\")\n",
        "        os.system(f\"rm part-{partName}.lzo.aria2\")\n",
        "        os.system(f\"rm part-{partName}.lzo.index\")\n",
        "        index+=1\n",
        "        gc.collect()\n",
        "os.system(\"mv ./temp.txt ./dictionary.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAXT2VHrouEs"
      },
      "source": [
        "#Code for modeling prediction error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzd7VhKnouE_"
      },
      "source": [
        "##NN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QdlPlyFouFA"
      },
      "source": [
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from math import ceil\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense,Activation,Dropout\n",
        "from keras.optimizers import RMSprop\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqNbwbQcouFC"
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "def create_weighted_binary_crossentropy(zero_weight, one_weight):\n",
        "    def weighted_binary_crossentropy(y_true, y_pred):\n",
        "        b_ce = K.binary_crossentropy(y_true, y_pred)\n",
        "        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n",
        "        weighted_b_ce = weight_vector * b_ce\n",
        "        return K.mean(weighted_b_ce)\n",
        "    return weighted_binary_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6V3C_UBouFC"
      },
      "source": [
        "def buildModel(inputSize,probabilistic=False):\n",
        "    model = Sequential()\n",
        "    size=32\n",
        "    reduction_factor=2\n",
        "    shape=(inputSize,)\n",
        "    depth=1\n",
        "    dropout=0.5\n",
        "    kernel_resularizer_norm=1e-5\n",
        "    for i in range(depth):\n",
        "                model.add(Dense(size,  input_shape=shape,kernel_regularizer=regularizers.l2(kernel_resularizer_norm)))\n",
        "                model.add(Activation(\"relu\"))\n",
        "                model.add(Dropout(dropout))\n",
        "                size=size//reduction_factor\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(Dense(8,kernel_regularizer=regularizers.l2(kernel_resularizer_norm)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1,kernel_regularizer=regularizers.l2(1e-4)))\n",
        "    if (probabilistic):\n",
        "        model.add(Activation(\"sigmoid\"))\n",
        "    else:\n",
        "        model.add(Activation(\"tanh\"))\n",
        "    model2=keras.Model(inputs=[model.inputs],outputs=model.layers[-3].output)\n",
        "\n",
        "    return (model,model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_KXWpAzouFD"
      },
      "source": [
        "import copy\n",
        "def computeLast(arr,last):\n",
        "    return last.predict(arr)\n",
        "\n",
        "from tqdm import tqdm\n",
        "def trainModel(train,trainTarget,test,testTarget,last=None):\n",
        "    if last!=None:\n",
        "        originTrain=copy.deepcopy(train)\n",
        "        originTest=copy.deepcopy(test)\n",
        "        originTarg=copy.deepcopy(trainTarget)\n",
        "        originTestTarg=copy.deepcopy(testTarget)\n",
        "        for m in tqdm(last, position=0, leave=True):\n",
        "            temp=computeLast(train,m)\n",
        "            trainTarget-=temp.ravel()\n",
        "            temp=computeLast(test,m)\n",
        "            testTarget-=temp.ravel()\n",
        "            gc.collect()\n",
        "        guessTe=originTestTarg-testTarget\n",
        "        del temp\n",
        "    count=len(train[0])\n",
        "    print(f\"features: {count}\")\n",
        "    model,model2=buildModel(count,probabilistic=(last==None))\n",
        "    BS=64\n",
        "    if last!=None:\n",
        "        loss=tf.keras.losses.MeanSquaredError(name=\"mse\")\n",
        "        metrics=[tf.keras.metrics.MeanAbsoluteError(\"mae\")]\n",
        "    else:\n",
        "        loss=create_weighted_binary_crossentropy(1,1.1)\n",
        "        metrics=[\"accuracy\",tf.keras.metrics.MeanAbsoluteError(\"mae\")]\n",
        "    model.compile(loss=loss, metrics=metrics,optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
        "    model.fit(train,trainTarget,epochs=1,batch_size=BS)\n",
        "    if last!=None:\n",
        "        del train\n",
        "        del trainTarget\n",
        "        del originTrain\n",
        "        del originTarg\n",
        "    gc.collect()\n",
        "    print(\"prediction....\")\n",
        "    prediction=model.predict(test)\n",
        "    if last!=None:\n",
        "        prediction=guessTe+prediction.ravel()\n",
        "        prediction=np.clip(prediction,0,1)\n",
        "        calculate_metrics(prediction,originTestTarg)\n",
        "    else:\n",
        "        calculate_metrics(prediction,testTarget)\n",
        "    return (model,model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_v7zmjrouFD"
      },
      "source": [
        "##Chunk wise download and work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpD3tDDYouFE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "994b9842-1f3f-43d8-fb14-6c947acb2f05"
      },
      "source": [
        "import os \n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "gc.collect()\n",
        "modelComp=[]\n",
        "modelParz=[]\n",
        "index=0\n",
        "with open(\"training_urls.txt\") as doc:\n",
        "    for line in doc:#tqdm(doc, position=0, leave=True):\n",
        "        #skip lzo.index files (odd row number)\n",
        "        if index%2==1:\n",
        "            index+=1\n",
        "            continue\n",
        "        os.system(f\"aria2c '{line}'\")\n",
        "        partName=str(index//2)\n",
        "        print(partName)\n",
        "        partName=\"0\"*(5-len(partName))+partName\n",
        "        \n",
        "        os.system(f'lzop -x part-{partName}.lzo')\n",
        "        os.system(f'mv part-{partName} train.csv')\n",
        "        os.system(f\"rm part-{partName}.lzo\")\n",
        "        train,trainTarget,test,testTarget=computeTrainTest()\n",
        "        if len(modelComp)==0:\n",
        "            model1,model2=trainModel(train,trainTarget,test,testTarget)\n",
        "        else:\n",
        "            model1,model2=trainModel(train,trainTarget,test,testTarget,last=modelComp)\n",
        "        modelComp.append(model1)\n",
        "        modelParz.append(model2)\n",
        "        os.system(\"rm train.csv\")\n",
        "        os.system(f\"rm part-{partName}.lzo.aria2\")\n",
        "        os.system(f\"rm part-{partName}.lzo.index\")\n",
        "        index+=1\n",
        "        gc.collect()\n",
        "os.system(\"mv ./temp.txt ./dictionary.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-142-d3d17881d521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'mv part-{partName} train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"rm part-{partName}.lzo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainTarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestTarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomputeTrainTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelComp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainTarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestTarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-137-d47d7fd35c5b>\u001b[0m in \u001b[0;36mcomputeTrainTest\u001b[0;34m(verbose)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#df[\"NoText\"]=df[\"text_tokens\"].apply(lambda row:  False if len(row.split(\"\\t\"))>2 else True).astype(\"uint16\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"distinct_text_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m\"\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uint16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"special_characters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountSpecial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uint16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-137-d47d7fd35c5b>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#df[\"NoText\"]=df[\"text_tokens\"].apply(lambda row:  False if len(row.split(\"\\t\"))>2 else True).astype(\"uint16\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"distinct_text_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m\"\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uint16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"special_characters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountSpecial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uint16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Ff_BXJ54TY"
      },
      "source": [
        "#Code for modeling prediction error giving different weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1zFs-Hm54UK"
      },
      "source": [
        "##NN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_HRaL1L54UL"
      },
      "source": [
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from math import ceil\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense,Activation,Dropout\n",
        "from keras.optimizers import RMSprop\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAHiHlsW54UN"
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "def create_weighted_binary_crossentropy(zero_weight, one_weight):\n",
        "    def weighted_binary_crossentropy(y_true, y_pred):\n",
        "        b_ce = K.binary_crossentropy(y_true, y_pred)\n",
        "        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n",
        "        weighted_b_ce = weight_vector * b_ce\n",
        "        return K.mean(weighted_b_ce)\n",
        "    return weighted_binary_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgO_FRI454UP"
      },
      "source": [
        "def buildModel(inputSize,probabilistic=False):\n",
        "    model = Sequential()\n",
        "    size=64\n",
        "    reduction_factor=2\n",
        "    shape=(inputSize,)\n",
        "    depth=2\n",
        "    dropout=0.4\n",
        "    BS=16\n",
        "    kernel_resularizer_norm=1e-5\n",
        "    for i in range(depth):\n",
        "        model.add(Dense(size,  input_shape=shape,kernel_regularizer=regularizers.l2(kernel_resularizer_norm)))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dropout(dropout))\n",
        "        size=size//reduction_factor\n",
        "    model.add(Dense(1,kernel_regularizer=regularizers.l2(1e-4)))\n",
        "\n",
        "    if (probabilistic):\n",
        "        model.add(Activation(\"sigmoid\"))\n",
        "    else:\n",
        "        pass\n",
        "        #model.add(Activation(\"tanh\"))\n",
        "    model2=keras.Model(inputs=[model.inputs],outputs=model.layers[-3].output)\n",
        "\n",
        "    return (model,model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjVNIY5y54UQ"
      },
      "source": [
        "import copy\n",
        "def computeLast(arr,last):\n",
        "    return last.predict(arr)\n",
        "\n",
        "from tqdm import tqdm\n",
        "def trainModel(train,trainTarget,test,testTarget,last=None,verbose=False):\n",
        "    i=0\n",
        "    alpha=0.75\n",
        "    lr=0.0003\n",
        "    if last!=None:\n",
        "        lr=0.001\n",
        "        originTrain=copy.deepcopy(train)\n",
        "        originTest=copy.deepcopy(test)\n",
        "        originTarg=copy.deepcopy(trainTarget)\n",
        "        originTestTarg=copy.deepcopy(testTarget)\n",
        "        \n",
        "        for m in tqdm(last, position=0, leave=True):\n",
        "            temp=computeLast(train,m)\n",
        "            trainTarget-=np.power(alpha,i)*temp.ravel()\n",
        "            temp=computeLast(test,m)\n",
        "            testTarget-=np.power(alpha,i)*temp.ravel()\n",
        "            gc.collect()\n",
        "            i+=1\n",
        "        guessTe=originTestTarg-testTarget\n",
        "        del temp\n",
        "    count=len(train[0])\n",
        "    print(f\"features: {count}\")\n",
        "    model,model2=buildModel(count,probabilistic=(last==None))\n",
        "    BS=64\n",
        "    if last!=None:\n",
        "        loss=tf.keras.losses.MeanSquaredError(name=\"mse\")\n",
        "        metrics=[tf.keras.metrics.MeanAbsoluteError(\"mae\")]\n",
        "    else:\n",
        "        loss=create_weighted_binary_crossentropy(1,1)\n",
        "        metrics=[\"accuracy\",tf.keras.metrics.MeanAbsoluteError(\"mae\")]\n",
        "    model.compile(loss=loss, metrics=metrics,optimizer=tf.keras.optimizers.Adam(learning_rate=lr))\n",
        "    model.fit(train,trainTarget,epochs=3,batch_size=BS)\n",
        "    if last!=None:\n",
        "        del train\n",
        "        del trainTarget\n",
        "        del originTrain\n",
        "        del originTarg\n",
        "    gc.collect()\n",
        "    print(\"prediction....\")\n",
        "    prediction=model.predict(test)\n",
        "    if last!=None:\n",
        "        prediction=guessTe+np.power(alpha,i)*prediction.ravel()\n",
        "        prediction=np.clip(prediction,0,1)\n",
        "        if verbose:\n",
        "            for el in prediction:\n",
        "                if el<0 or el>1:\n",
        "                    print(el)\n",
        "        calculate_metrics(prediction,originTestTarg)\n",
        "    else:\n",
        "        calculate_metrics(prediction,testTarget)\n",
        "    return (model,model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XpMDjFh54UR"
      },
      "source": [
        "##Chunk wise download and work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNEG-Ogg54US"
      },
      "source": [
        "import os \n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "gc.collect()\n",
        "modelComp=[]\n",
        "modelParz=[]\n",
        "with open(\"training_urls.txt\") as doc:\n",
        "    index=0\n",
        "    for line in doc:#tqdm(doc, position=0, leave=True):\n",
        "        #skip lzo.index files (odd row number)\n",
        "        if index%2==1:\n",
        "            index+=1\n",
        "            continue\n",
        "        os.system(f\"aria2c '{line}'\")\n",
        "        partName=str(index//2)\n",
        "        print(partName)\n",
        "        partName=\"0\"*(5-len(partName))+partName\n",
        "        \n",
        "        os.system(f'lzop -x part-{partName}.lzo')\n",
        "        os.system(f'mv part-{partName} train.csv')\n",
        "        os.system(f\"rm part-{partName}.lzo\")\n",
        "        train,trainTarget,test,testTarget=computeTrainTest()\n",
        "        if len(modelComp)==0:\n",
        "            model1,model2=trainModel(train,trainTarget,test,testTarget)\n",
        "        else:\n",
        "            model1,model2=trainModel(train,trainTarget,test,testTarget,last=modelComp)\n",
        "        modelComp.append(model1)\n",
        "        modelParz.append(model2)\n",
        "        os.system(\"rm train.csv\")\n",
        "        os.system(f\"rm part-{partName}.lzo.aria2\")\n",
        "        os.system(f\"rm part-{partName}.lzo.index\")\n",
        "        index+=1\n",
        "        gc.collect()\n",
        "os.system(\"mv ./temp.txt ./dictionary.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQt2iPow52O-"
      },
      "source": [
        "summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg9Se23uIFSU"
      },
      "source": [
        "train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ruIV64b7N0X"
      },
      "source": [
        "train,trainTarget,test,testTarget=computeTrainTest()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxXsUkKSjc3e"
      },
      "source": [
        "originTrain=copy.deepcopy(train)\n",
        "originTest=copy.deepcopy(test)\n",
        "originTarg=copy.deepcopy(trainTarget)\n",
        "originTestTarg=copy.deepcopy(testTarget)\n",
        "i=0\n",
        "alpha=0.75        \n",
        "for m in tqdm(modelComp[:3], position=0, leave=True):\n",
        "    temp=computeLast(train,m)\n",
        "    trainTarget-=np.power(alpha,i)*temp.ravel()\n",
        "    temp=computeLast(test,m)\n",
        "    testTarget-=np.power(alpha,i)*temp.ravel()\n",
        "    gc.collect()\n",
        "    i+=1\n",
        "guessTe=originTestTarg-testTarget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpYWlm1ojuDG"
      },
      "source": [
        "guessTr=originTarg-trainTarget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx13jVe4jsow"
      },
      "source": [
        "calculate_metrics(guessTr,originTarg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbqgtV1_o0JY"
      },
      "source": [
        "rce=7.2338845972432\n",
        "AP=0.5553876611554468\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Wn0ZToQnwpc"
      },
      "source": [
        "i=0\n",
        "for model in modelComp[:3]:\n",
        "    model.save(f\"baseNet-{targetCol}-{i}-{alpha}.h5\")\n",
        "    i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "906lSt5NobTY"
      },
      "source": [
        "summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ElWpjxwof6a"
      },
      "source": [
        "Summary:\n",
        "\n",
        "\n",
        "    {'engaged_with_user_follower_count': {'mean': 842019.06, 'std': 4673210.5},\n",
        "    'engaged_with_user_following_count': {'mean': 3866.4478,\n",
        "    'std': 34336.34765625},\n",
        "    'engaged_with_user_is_verified': {'mean': 0.2052318,\n",
        "    'std': 0.39793893694877625},\n",
        "    'engagee_follows_engager': {'mean': 0.40177855, 'std': 0.49429482221603394},\n",
        "    'engaging_user_follower_count': {'mean': 816.23035, 'std': 10675.9775390625},\n",
        "    'engaging_user_following_count': {'mean': 716.2875,\n",
        "    'std': 1651.0892333984375},\n",
        "    'engaging_user_is_verified': {'mean': 0.0018827069,\n",
        "    'std': 0.04330842196941376},\n",
        "    'gifs': {'mean': 0.012480928, 'std': 0.11041624844074249},\n",
        "    'hashtags': {'mean': 0.41888434, 'std': 1.146403431892395},\n",
        "    'photos': {'mean': 0.47115853, 'std': 0.8963454961776733},\n",
        "    'present_domains': {'mean': 0.1264995, 'std': 0.36130860447883606},\n",
        "    'present_links': {'mean': 0.1264995, 'std': 0.36130860447883606},\n",
        "    'present_media': {'mean': 0.5532317, 'std': 0.8825932741165161},\n",
        "    'text_tokens': {'mean': 46.921314, 'std': 29.6999454498291},\n",
        "    'tweet_type_Quote': {'mean': 0.08192547, 'std': 0.2696485221385956},\n",
        "    'tweet_type_Retweet': {'mean': 0.32451233, 'std': 0.468744695186615},\n",
        "    'tweet_type_TopLevel': {'mean': 0.5935622, 'std': 0.49659907817840576},\n",
        "    'videos': {'mean': 0.069592245, 'std': 0.25196176767349243}}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5C4-BKNoIGj"
      },
      "source": [
        "import json\n",
        "with open('summary.json', 'w') as fp:\n",
        "    json.dump(summary, fp)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}