{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Representation Learner Chuncks .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "67w3_D5WbwAP",
        "qRG3snsqbwAR",
        "J8L7S5bjkCel",
        "hxzJTJLMbwAV",
        "Vctg7k-kn5R7",
        "cVmIE-nsvFO2",
        "uMuIdfPoZmEj",
        "RBpXMIEmvK45",
        "TVpdjaVI8EQu"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Wrq1qypev6"
      },
      "source": [
        "%%capture\n",
        "!apt-get install  aria2\n",
        "!apt-get install lzop\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIkd0HIYXT59"
      },
      "source": [
        "%%capture\n",
        "import os, json, zipfile, shutil, platform, time\n",
        "\n",
        "import scipy.sparse as sps\n",
        "from pandas import DataFrame\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import multiprocessing\n",
        "import multiprocessing.pool\n",
        "import time\n",
        "\n",
        "from random import randint\n",
        "from tensorflow import keras\n",
        "targetCol=\"like\"\n",
        "targets=[\"like\",\t\"retweet\",\t\"retweetCom\",\t\"reply\",\t\"interaction\"]\n",
        "droppable=[t for t in targets if t!=targetCol]\n",
        "import gc\n",
        "import numpy as np\n",
        "np.random.seed(1234)\n",
        "gc.collect()\n",
        "\n",
        "#Disable copy warning (hopefully are false positive here)\n",
        "pd.options.mode.chained_assignment = None"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsQa1_8ZgSt3"
      },
      "source": [
        "all_features = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\\\n",
        "                \"tweet_type\",\"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\\\n",
        "               \"engaged_with_user_following_count\", \"engaged_with_user_is_verified\", \"engaged_with_user_account_creation\",\\\n",
        "               \"engaging_user_id\", \"engaging_user_follower_count\", \"engaging_user_following_count\", \"engaging_user_is_verified\",\\\n",
        "               \"engaging_user_account_creation\", \"engagee_follows_engager\",\"reply_timestamp\", \"retweet_timestamp\", \"retweet_with_comment_timestamp\", \"like_timestamp\"]\n",
        "\n",
        "all_features_to_idx = dict(zip(all_features, range(len(all_features))))\n",
        "idx_to_features=dict(zip(range(len(all_features)),all_features))\n",
        "labels_to_idx = {\"reply_timestamp\": 20, \"retweet_timestamp\": 21, \"retweet_with_comment_timestamp\": 22, \"like_timestamp\": 23}\n",
        "list_of_dict=[]\n",
        "for i in range(len(all_features)):\n",
        "    list_of_dict.append({})"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67w3_D5WbwAP"
      },
      "source": [
        "# Preprocessing function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRG3snsqbwAR"
      },
      "source": [
        "###Special Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjT6WteXbwAS"
      },
      "source": [
        "HTTPS_token = '14120' \n",
        "RT_token = '56898'\n",
        "at_token = '137'\n",
        "hashtag_token = '108'\n",
        "gt_token = '135'\n",
        "lt_token = '133'\n",
        "amp_token = '111'\n",
        "question_token = '136'\n",
        "esclamation_token = '106'\n",
        "period_token = '119'\n",
        "two_periods_token = '131'\n",
        "coma_token = '117'\n",
        "dollar_token = '109'\n",
        "period_coma_token = '132'\n",
        "parenthesis_open_token = '113'\n",
        "parenthesis_closed_token = '114'\n",
        "star_token = '115'\n",
        "slash_token = '120'\n",
        "line_token = '118'\n",
        "underscore_token = '168'\n",
        "tilde_token = '198'\n",
        "virgolette_token = '107'\n",
        "square_parenthesis_open_token = '164'\n",
        "square_parenthesis_closed_token = '166'\n",
        "unk_token = '100'\n",
        "others_tokens = ['11733', '12022']\n",
        "twitter_internal_link=\"188\" #\"t\":188,#twitter links\n",
        "co_domain_token= \"11170\" #\"co\":11170\n",
        "eight=\"129\" #\"8\":129 ???? forse utile\n",
        "\n",
        "special_tokens_list = [\n",
        "    at_token,\n",
        "    hashtag_token,\n",
        "    gt_token,\n",
        "    lt_token,\n",
        "    amp_token,\n",
        "    question_token,\n",
        "    esclamation_token,\n",
        "    period_token,\n",
        "    coma_token,\n",
        "    dollar_token,\n",
        "    period_coma_token,\n",
        "    two_periods_token,\n",
        "    parenthesis_open_token,\n",
        "    parenthesis_closed_token,\n",
        "    star_token,\n",
        "    slash_token,\n",
        "    line_token,\n",
        "    underscore_token,\n",
        "    tilde_token,\n",
        "    virgolette_token,\n",
        "    square_parenthesis_open_token,\n",
        "    square_parenthesis_closed_token,\n",
        "    unk_token,\n",
        "    twitter_internal_link, \n",
        "    co_domain_token,\n",
        "    eight, \n",
        "]\n",
        "\n",
        "jimin=\"27128\"\n",
        "V=\"159\"\n",
        "BTS=\"BTS\"\n",
        "btsTokenized=\"70447\\t10731\\t\"\n",
        "BE=\"46291\" #album name\n",
        "bts=[\n",
        "    jimin,\n",
        "    V,\n",
        "    BE,\n",
        "    BTS\n",
        "]\n",
        "\n",
        "EA=\"38478\"\n",
        "SPORTS=\"15506\"\n",
        "\n",
        "sport=[\n",
        "    EA,\n",
        "    SPORTS,\n",
        "]\n",
        "\n",
        "quarterback=\"154\"\n",
        "ncaa=\"23864\"\n",
        "FOOTBALL=\"12499\"\n",
        "nfl=\"20179\"\n",
        "Denard_Robinson=[11274\t,10235]\n",
        "football=[\n",
        "        *sport,\n",
        "        quarterback,\n",
        "        ncaa,\n",
        "        FOOTBALL,\n",
        "        nfl\n",
        "]\n",
        "def countList(list_specific):\n",
        "    def count_f(row):\n",
        "        row=row.split(\"\\t\")\n",
        "        count=0\n",
        "        for el in row:\n",
        "            if el in list_specific:\n",
        "                count+=1\n",
        "        return count\n",
        "    return count_f\n",
        "\n",
        "countSpecial=countList(special_tokens_list)\n",
        "\n",
        "countBTS=countList(bts)\n",
        "\n",
        "countSports=countList(sport)\n",
        "\n",
        "countFootball=countList(football)\n",
        "\n",
        "\n",
        "def countSpecificWord(word):\n",
        "    def count_f(row):\n",
        "        row=row.split(\"\\t\")\n",
        "        return row.count(word)\n",
        "    return count_f\n",
        "\n",
        "def countWordSet(words):\n",
        "    def count_f(row):\n",
        "        row=row.split(\"\\t\")\n",
        "        count=0\n",
        "        for w in words:\n",
        "            count+=row.count(w)\n",
        "        return w\n",
        "    return count_f\n",
        "\n",
        "def checkBalanced(a,b):\n",
        "    def count_f(row):\n",
        "        row=row.split(\"\\t\")\n",
        "        counta=row.count(a)\n",
        "        countb=row.count(b)\n",
        "        return 1 if counta==countb else 0\n",
        "    return count_f\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOF2DxSUbwAU"
      },
      "source": [
        "def removeRT(row):\n",
        "    if row.find(\"101\\t56898\")!=-1:\n",
        "        idx=row.find(\"131\")\n",
        "        return row[:3]+row[idx+3:]\n",
        "    return row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8L7S5bjkCel"
      },
      "source": [
        "###Hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87E9RLLpZQZs"
      },
      "source": [
        "origin=\"drive/MyDrive/twitter/\"\n",
        "files=[\"ErdoganHashtags.bz2\",\"BigBrotherHashtags.bz2\",\n",
        "       \"BTSHashtags.bz2\",\"NCTHashtags.bz2\",\"GOT7Hashtags.bz2\",\"ThaiHashtags.bz2\",\n",
        "       \"BirmaniaHashtags.bz2\",\"CovidHashtags.bz2\",\"ImpactHashtags.bz2\",\"PeingHashtags.bz2\"]\n",
        "res=[]\n",
        "import json\n",
        "\n",
        "for file in files:\n",
        "    os.system(f\"cp {origin+file} {file}\")\n",
        "    os.system(f\"bunzip2 {file}\")\n",
        "    os.system(f\"mv {file[:-4]} {file[:-4]}.json\")\n",
        "    with open(f'{file[:-4]}.json') as f:\n",
        "        data = json.load(f)\n",
        "        res.append(set(data.keys()))\n",
        "        del data\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "countHashtagFunctions=[]\n",
        "for i in range(len(files)):\n",
        "    countHashtagFunctions.append([f\"count_{i}\",countList(res[i])])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxzJTJLMbwAV"
      },
      "source": [
        "###actual code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xock9leQbwAW"
      },
      "source": [
        "def countMedia(arr,mediaType):\n",
        "        c=0\n",
        "        for m in arr:\n",
        "            if m==mediaType:\n",
        "                c+=1\n",
        "        return c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdzv_iUebwAW"
      },
      "source": [
        "import re\n",
        "summary={}\n",
        "def computeTrainTest(verbose=False,userSpecific=False):\n",
        "    df=pd.read_csv(\"train.csv\",header=None,sep='\\x01')\n",
        "    df.columns=all_features\n",
        "\n",
        "    df[\"like\"]=df[\"like_timestamp\"].apply(lambda row: 1 if row>0 else 0).astype(\"uint8\")\n",
        "    df.drop(\"like_timestamp\",axis=1,inplace=True)\n",
        "\n",
        "    df[\"retweet\"]=df[\"retweet_timestamp\"].apply(lambda row: 1 if row>0 else 0).astype(\"uint8\")\n",
        "    df.drop(\"retweet_timestamp\",axis=1,inplace=True)\n",
        "\n",
        "    df[\"retweetCom\"]=df[\"retweet_with_comment_timestamp\"].apply(lambda row: 1 if row>0 else 0).astype(\"uint8\")\n",
        "    df.drop(\"retweet_with_comment_timestamp\",axis=1,inplace=True)\n",
        "\n",
        "    df[\"reply\"]=df[\"reply_timestamp\"].apply(lambda row: 1 if row>0 else 0).astype(\"uint8\")\n",
        "    df.drop(\"reply_timestamp\",axis=1,inplace=True)\n",
        "\n",
        "    df[\"interaction\"]=df[\"reply\"]+df[\"retweetCom\"]+df[\"retweet\"]+df[\"like\"]\n",
        "\n",
        "    df[\"interaction\"][df[\"interaction\"]>0]=1\n",
        "\n",
        "\n",
        "    df.fillna(value={\"present_media\":\"\"},inplace=True)\n",
        "    df[\"photos\"]=df[\"present_media\"].apply(lambda row: countMedia((row.split(\"\\t\")),\"Photo\")).astype(\"uint8\")\n",
        "    df[\"gifs\"]=df[\"present_media\"].apply(lambda row: countMedia((row.split(\"\\t\")),\"GIF\")).astype(\"uint8\")\n",
        "    df[\"videos\"]=df[\"present_media\"].apply(lambda row: countMedia((row.split(\"\\t\")),\"Video\")).astype(\"uint8\")\n",
        "\n",
        "    #df[\"NoText\"]=df[\"text_tokens\"].apply(lambda row:  False if len(row.split(\"\\t\"))>2 else True).astype(\"uint16\")\n",
        "\n",
        "    df[\"distinct_text_tokens\"]=df[\"text_tokens\"].apply(lambda row: len(set(row.split(\"\\t\")))-2 if row!=\"\" else 0).astype(\"uint16\")\n",
        "\n",
        "    df[\"special_characters\"]=df[\"text_tokens\"].apply(countSpecial).astype(\"uint16\")\n",
        "\n",
        "    df[\"text_tokens\"]=df[\"text_tokens\"].apply(lambda row:removeRT(row))\n",
        "\n",
        "    #remove links\n",
        "    df[\"text_tokens\"]=df[\"text_tokens\"].apply(lambda row: re.sub(\"14120\\t131\\t120\\t120\\t188\\t119\\t11170\\t120\\t\\d+\\t\\d+\\t\\d+\\t\\d+\\t\\d+\\t\\d+\\t\\d+\\t\\d+\\t\",\"\",row) )\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    df[\"text_tokens\"]=df[\"text_tokens\"].apply(lambda row: re.sub(\"70447\\t10731\\t\",\"BTS\",row))\n",
        "\n",
        "    df[\"isReplying\"]=df[\"text_tokens\"].apply(lambda row: row.startswith(\"101\\t137\"))\n",
        "\n",
        "    df[\"unknown\"]=df[\"text_tokens\"].apply(countSpecificWord(unk_token)).astype(\"uint16\")\n",
        "    \n",
        "    #df[\"sentence\"]=df[\"text_tokens\"].apply(countSpecificWord(period_token)).astype(\"uint16\")\n",
        "\n",
        "    df[\"esclamations\"]=df[\"text_tokens\"].apply(countSpecificWord(esclamation_token)).astype(\"uint16\")\n",
        "\n",
        "    df[\"questions\"]=df[\"text_tokens\"].apply(countSpecificWord(question_token)).astype(\"uint16\")\n",
        "\n",
        "    df[\"mentions\"]=df[\"text_tokens\"].apply(countSpecificWord(at_token)).astype(\"uint16\")\n",
        "\n",
        "    df[\"twitter_Specific\"]=df[\"text_tokens\"].apply(countWordSet([RT_token,at_token,hashtag_token])).astype(\"uint16\")\n",
        "\n",
        "    #RT token at the beginning is removed so remaining RT tokens should indicate a request for retweet\n",
        "    df[\"asking_retwet\"]=df[\"text_tokens\"].apply(countWordSet([RT_token])).astype(\"uint16\")\n",
        "\n",
        "    df[\"ask_reply\"]=df[\"text_tokens\"].apply(lambda row: row.count(\"76456\\t10454\"))\n",
        "\n",
        "    #Noisy the word like has other meanings\n",
        "    df[\"ask_like\"]=df[\"text_tokens\"].apply(lambda row: row.count(\"11850\"))\n",
        "\n",
        "\n",
        "    df[\"subsentence\"]=df[\"text_tokens\"].apply(countSpecificWord(coma_token)).astype(\"uint16\")\n",
        "\n",
        "    df[\"subsentence_semantic_separation\"]=df[\"text_tokens\"].apply(countSpecificWord(period_coma_token)).astype(\"uint16\")\n",
        "\n",
        "    #df[\"BTS\"]=df[\"text_tokens\"].apply(countBTS).astype(\"uint16\")\n",
        "\n",
        "    #df[\"Sports\"]=df[\"text_tokens\"].apply(countSports).astype(\"uint16\")\n",
        "\n",
        "    #df[\"Football\"]=df[\"text_tokens\"].apply(countFootball).astype(\"uint16\")\n",
        "\n",
        "    #df[\"proportion\"]=df.apply(lambda row: row[\"distinct_text_tokens\"]/max(1,row[\"special_characters\"]))\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    #df[\"balanced_angular\"]=df[\"text_tokens\"].apply(checkBalanced(gt_token,lt_token)).astype(\"uint16\")\n",
        "\n",
        "    #df[\"balanced_square\"]=df[\"text_tokens\"].apply(checkBalanced(square_parenthesis_open_token,square_parenthesis_closed_token)).astype(\"uint16\")\n",
        "\n",
        "    #df[\"balanced_round\"]=df[\"text_tokens\"].apply(checkBalanced(parenthesis_open_token,parenthesis_closed_token,)).astype(\"uint16\")\n",
        "\n",
        "    df[\"parenthesis\"]=df[\"text_tokens\"].apply(countWordSet([parenthesis_open_token,parenthesis_closed_token,gt_token,lt_token,square_parenthesis_open_token,square_parenthesis_closed_token])).astype(\"uint16\")\n",
        "    \n",
        "    \n",
        "\n",
        "    df[\"text_tokens\"]=df[\"text_tokens\"].apply(lambda row: len(row.split(\"\\t\")) if row!=\"\" else 0).astype(\"uint16\")\n",
        "    df.fillna(value={\"hashtags\":\"\"},inplace=True)\n",
        "    \n",
        "    \n",
        "\n",
        "    df[\"present_media\"]=df[\"present_media\"].apply(lambda row: len(row.split(\"\\t\")) if row!=\"\" else 0).astype(\"uint8\")\n",
        "    df.fillna(value={\"present_links\":\"\"},inplace=True)\n",
        "    df[\"present_links\"]=df[\"present_links\"].apply(lambda row: len(row.split(\"\\t\")) if row!=\"\" else 0).astype(\"uint8\")\n",
        "    df.fillna(value={\"present_domains\":\"\"},inplace=True)\n",
        "    df[\"present_domains\"]=df[\"present_domains\"].apply(lambda row: len(row.split(\"\\t\")) if row!=\"\" else 0).astype(\"uint8\")\n",
        "    \n",
        "    df=pd.get_dummies(df,columns=[\"tweet_type\"])\n",
        "\n",
        "    gc.collect()\n",
        "    df.drop(\"tweet_id\",axis=1,inplace=True)\n",
        "    df.drop(\"engaging_user_account_creation\",axis=1,inplace=True)\n",
        "    df.drop(\"engaged_with_user_account_creation\",axis=1,inplace=True)\n",
        "    df.drop(\"tweet_timestamp\",axis=1,inplace=True)\n",
        "\n",
        "    df.drop(\"engaged_with_user_id\",axis=1,inplace=True)\n",
        "    \n",
        "    df.drop(\"language\",axis=1,inplace=True)\n",
        "    \n",
        "    \n",
        "    #TODO Normalization\n",
        "    msk = np.random.rand(len(df)) < 0.8\n",
        "    train=df[msk]\n",
        "    test=df[~msk]\n",
        "    \n",
        "    hashtagCols=[]\n",
        "    print(\"here\")\n",
        "    for f in countHashtagFunctions:\n",
        "        hashtagCols.append(f[0])\n",
        "        train[f[0]]=train[\"hashtags\"].apply(f[1]).astype(\"uint8\")\n",
        "        test[f[0]]=test[\"hashtags\"].apply(f[1]).astype(\"uint8\")\n",
        "        \n",
        "    if userSpecific:\n",
        "        tempTable=train[train[\"interaction\"]>0].copy()\n",
        "\n",
        "        tempTable=tempTable[[*hashtagCols,\"engaging_user_id\"]]\n",
        "\n",
        "        tempTable=tempTable.groupby([\"engaging_user_id\"]).sum()\n",
        "\n",
        "        trainTable=tempTable.copy()\n",
        "        testTable=tempTable.copy()\n",
        "        for c in hashtagCols:\n",
        "            trainTable[c]=trainTable[c].apply(lambda row: 0 if row<2 else 1)\n",
        "            testTable[c]=testTable[c].apply(lambda row: 0 if row<1 else 1)\n",
        "        if verbose:\n",
        "            print(trainTable[:20])\n",
        "        train=train.merge(trainTable,how=\"left\",on=\"engaging_user_id\")\n",
        "        \n",
        "        test=test.merge(testTable,how=\"left\",on=\"engaging_user_id\")\n",
        "        \n",
        "\n",
        "    train.fillna(0,inplace=True)\n",
        "    test.fillna(0,inplace=True)\n",
        "    train.drop(\"engaging_user_id\",axis=1,inplace=True)\n",
        "    test.drop(\"engaging_user_id\",axis=1,inplace=True)\n",
        "\n",
        "    train[\"hashtags\"]=train[\"hashtags\"].apply(lambda row: len(row.split(\"\\t\")) if row!=\"\" else 0).astype(\"uint8\")\n",
        "    test[\"hashtags\"]=test[\"hashtags\"].apply(lambda row: len(row.split(\"\\t\")) if row!=\"\" else 0).astype(\"uint8\")\n",
        "\n",
        "    for col in train.columns:\n",
        "        if col  in droppable:\n",
        "            train.drop(col,axis=1,inplace=True)\n",
        "            test.drop(col,axis=1,inplace=True)\n",
        "        elif col in [\"engaged_with_user_id\",\"engaging_user_id\",\"language\"]:\n",
        "            train.drop(col,axis=1,inplace=True)\n",
        "            test.drop(col,axis=1,inplace=True)\n",
        "        else:\n",
        "            train[col]=train[col].astype(\"float32\")\n",
        "            test[col]=test[col].astype(\"float32\")\n",
        "    for col in train.columns:\n",
        "        if col!=targetCol:\n",
        "            if col not in summary:\n",
        "                summary[col]={\"mean\":np.mean(train[col].values),\"std\":np.std(train[col])}\n",
        "            if summary[col][\"std\"]==0:\n",
        "                train[col]=0\n",
        "                test[col]=0\n",
        "            else:\n",
        "                train[col]=(train[col]-summary[col][\"mean\"])/summary[col][\"std\"]\n",
        "                test[col]=(test[col]-summary[col][\"mean\"])/summary[col][\"std\"]\n",
        "\n",
        "\n",
        "    target=train[targetCol]\n",
        "    target=target.astype(\"float32\")\n",
        "    target=target.values\n",
        "    targetTest=test[targetCol]\n",
        "    targetTest=targetTest.astype(\"float32\")\n",
        "    targetTest=targetTest.values\n",
        "    train.drop(targetCol,axis=1,inplace=True)\n",
        "    test.drop(targetCol,axis=1,inplace=True)\n",
        "    input=train.values\n",
        "    inputTest=test.values\n",
        "    del df\n",
        "    del train\n",
        "    del test\n",
        "    return (input,target,inputTest,targetTest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vctg7k-kn5R7"
      },
      "source": [
        "#Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAlpu6Lrn6QI"
      },
      "source": [
        "from sklearn.metrics import average_precision_score, log_loss\n",
        "def calculate_ctr(gt):\n",
        "    positive = len([x for x in gt if x == 1])\n",
        "    ctr = positive/float(len(gt))\n",
        "    return ctr\n",
        "\n",
        "def compute_rce(pred, gt):\n",
        "    cross_entropy = log_loss(gt, pred-1e-7)\n",
        "    data_ctr = calculate_ctr(gt)\n",
        "    strawman_cross_entropy = log_loss(gt, [data_ctr for _ in range(len(gt))])\n",
        "    return (1.0 - cross_entropy/(strawman_cross_entropy+1e-7))*100.0\n",
        "\n",
        "def calculate_metrics(prediction,targetTest):\n",
        "    rce = compute_rce(prediction,targetTest)\n",
        "    average_precision = average_precision_score(targetTest,prediction)\n",
        "    print(f\"rce={rce}\")\n",
        "    print(f\"AP={average_precision}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVmIE-nsvFO2"
      },
      "source": [
        "#Use last layers output as input to next layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuIdfPoZmEj"
      },
      "source": [
        "##NN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auavUQ8_f-l7"
      },
      "source": [
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from math import ceil\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense,Activation,Dropout\n",
        "from keras.optimizers import RMSprop\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slc-tix2f9MA"
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "def create_weighted_binary_crossentropy(zero_weight, one_weight):\n",
        "    def weighted_binary_crossentropy(y_true, y_pred):\n",
        "        b_ce = K.binary_crossentropy(y_true, y_pred)\n",
        "        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n",
        "        weighted_b_ce = weight_vector * b_ce\n",
        "        return K.mean(weighted_b_ce)\n",
        "    return weighted_binary_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPbDlM1PgBBF"
      },
      "source": [
        "def buildModel(inputSize):\n",
        "    model = Sequential()\n",
        "    size=64\n",
        "    repSize=64\n",
        "    reduction_factor=2\n",
        "    shape=(inputSize,)\n",
        "    depth=1\n",
        "    dropout=0.4\n",
        "    kernel_resularizer_norm=1e-5\n",
        "    for i in range(depth):\n",
        "                model.add(Dense(size,  input_shape=shape,kernel_regularizer=regularizers.l2(kernel_resularizer_norm)))\n",
        "                model.add(Activation(\"relu\"))\n",
        "                model.add(Dropout(dropout))\n",
        "                size=size//reduction_factor\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(Dense(repSize,kernel_regularizer=regularizers.l2(kernel_resularizer_norm)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Dropout(dropout))\n",
        "    #model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(Dense(1,kernel_regularizer=regularizers.l2(1e-4)))\n",
        "    model.add(Activation(\"sigmoid\"))\n",
        "    model2=keras.Model(inputs=[model.inputs],outputs=model.layers[-3].output)\n",
        "\n",
        "    return (model,model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azV5dTs_ZnX2"
      },
      "source": [
        "import copy\n",
        "def computeLast(arr,last):\n",
        "    return last.predict(arr)\n",
        "\n",
        "from tqdm import tqdm\n",
        "def trainModel(train,trainTarget,test,testTarget,last=None):\n",
        "    if last!=None:\n",
        "        for m in tqdm(last, position=0, leave=True):\n",
        "            train=computeLast(train,m)\n",
        "            test=computeLast(test,m)\n",
        "            gc.collect()\n",
        "    count=len(train[0])\n",
        "    print(f\"features: {count}\")\n",
        "    model,model2=buildModel(count)\n",
        "    BS=64\n",
        "    #loss=create_weighted_binary_crossentropy(1,1.2)\n",
        "    loss= tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    #loss= tf.keras.losses.MeanSquaredError() 5.292315388887181\n",
        "    # =>NO=>loss= tf.keras.losses.MeanAbsoluteError()\n",
        "    #loss= tf.keras.losses.Huber() #5.5623\n",
        "    #loss= tf.keras.losses.LogCosh()#5.0146473242150265\n",
        "    # => NO => loss= tf.keras.losses.Hinge()\n",
        "    # => NO => loss= tf.keras.losses.SquaredHinge()\n",
        "    # => NO =>loss= tf.keras.losses.CategoricalHinge()\n",
        "    #loss= tf.keras.losses.MeanSquaredLogarithmicError() 4.363344166677519\n",
        "    # => NO => loss= tf.keras.losses.KLDivergence()\n",
        "    #loss= tf.keras.losses.Poisson() #4.717\n",
        "    model.compile(loss=loss, metrics=[tf.keras.metrics.AUC(),\"accuracy\"],optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001))\n",
        "    model.fit(train,trainTarget,epochs=2,batch_size=BS)\n",
        "    prediction=model.predict(test)\n",
        "    calculate_metrics(prediction,testTarget)\n",
        "    return (model,model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBpXMIEmvK45"
      },
      "source": [
        "##Chunk wise download and work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paaBAbtPvNlv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "outputId": "35db682b-3944-4403-ccdd-533c2f4ed27a"
      },
      "source": [
        "import os \n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "gc.collect()\n",
        "modelComp=[]\n",
        "modelParz=[]\n",
        "\n",
        "with open(\"training_urls.txt\") as doc:\n",
        "    index=0\n",
        "    for line in doc:#tqdm(doc, position=0, leave=True):\n",
        "        #skip lzo.index files (odd row number)\n",
        "        if index%2==1:\n",
        "            index+=1\n",
        "            continue\n",
        "        os.system(f\"aria2c '{line}'\")\n",
        "        partName=str(index//2)\n",
        "        print(partName)\n",
        "        partName=\"0\"*(5-len(partName))+partName\n",
        "        \n",
        "        os.system(f'lzop -x part-{partName}.lzo')\n",
        "        os.system(f'mv part-{partName} train.csv')\n",
        "        os.system(f\"rm part-{partName}.lzo\")\n",
        "        train,trainTarget,test,testTarget=computeTrainTest()\n",
        "        if len(modelComp)==0:\n",
        "            model1,model2=trainModel(train,trainTarget,test,testTarget)\n",
        "        else:\n",
        "            model1,model2=trainModel(train,trainTarget,test,testTarget,last=modelParz)\n",
        "        modelComp.append(model1)\n",
        "        modelParz.append(model2)\n",
        "        os.system(\"rm train.csv\")\n",
        "        os.system(f\"rm part-{partName}.lzo.aria2\")\n",
        "        os.system(f\"rm part-{partName}.lzo.index\")\n",
        "        index+=1\n",
        "        gc.collect()\n",
        "os.system(\"mv ./temp.txt ./dictionary.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "features: 36\n",
            "Epoch 1/2\n",
            "41033/41033 [==============================] - 110s 3ms/step - loss: 0.7538 - auc: 0.5792 - accuracy: 0.5728\n",
            "Epoch 2/2\n",
            "41033/41033 [==============================] - 117s 3ms/step - loss: 0.6495 - auc: 0.6401 - accuracy: 0.6249\n",
            "rce=6.504797459177714\n",
            "AP=0.5553265802771993\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [01:08<00:00, 68.17s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "features: 64\n",
            "Epoch 1/2\n",
            "41010/41010 [==============================] - 103s 2ms/step - loss: 0.6919 - auc_1: 0.6069 - accuracy: 0.6067\n",
            "Epoch 2/2\n",
            "41010/41010 [==============================] - 120s 3ms/step - loss: 0.6395 - auc_1: 0.6560 - accuracy: 0.6348\n",
            "rce=6.7144799832604125\n",
            "AP=0.5557232841580829\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [02:23<00:00, 71.86s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "features: 64\n",
            "Epoch 1/2\n",
            "41005/41005 [==============================] - 112s 3ms/step - loss: 0.7311 - auc_2: 0.6081 - accuracy: 0.5763\n",
            "Epoch 2/2\n",
            "41005/41005 [==============================] - 110s 3ms/step - loss: 0.6380 - auc_2: 0.6589 - accuracy: 0.6373\n",
            "rce=6.4582958520764455\n",
            "AP=0.5532737829052656\n",
            "3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-bc167681c283>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'mv part-{partName} train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"rm part-{partName}.lzo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainTarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestTarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomputeTrainTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelComp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainTarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestTarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-d47d7fd35c5b>\u001b[0m in \u001b[0;36mcomputeTrainTest\u001b[0;34m(verbose)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"BTS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountBTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uint16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Sports\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountSports\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uint16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-ca20644ad3ed>\u001b[0m in \u001b[0;36mcount_f\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_specific\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0mcount\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiTRhJud8EQj"
      },
      "source": [
        "#Just one model => train on successive chuncks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVpdjaVI8EQu"
      },
      "source": [
        "##NN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhFgYy3h8EQu"
      },
      "source": [
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from math import ceil\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense,Activation,Dropout\n",
        "from keras.optimizers import RMSprop\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cULgDh8M8EQw"
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "def create_weighted_binary_crossentropy(zero_weight, one_weight):\n",
        "    def weighted_binary_crossentropy(y_true, y_pred):\n",
        "        b_ce = K.binary_crossentropy(y_true, y_pred)\n",
        "        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n",
        "        weighted_b_ce = weight_vector * b_ce\n",
        "        return K.mean(weighted_b_ce)\n",
        "    return weighted_binary_crossentropy"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh9HbEm78EQy"
      },
      "source": [
        "def buildModel(inputSize,bias=None):\n",
        "    if bias is not None:\n",
        "        bias = tf.keras.initializers.Constant(bias)\n",
        "    if targetCol==\"like\":\n",
        "        model = Sequential()\n",
        "        size=128\n",
        "        reduction_factor=2\n",
        "        shape=(inputSize,)\n",
        "        depth=3\n",
        "        dropout=0.4\n",
        "        kernel_resularizer_norm=1e-4\n",
        "        for i in range(depth):\n",
        "                model.add(Dense(size,  input_shape=shape,kernel_regularizer=regularizers.l2(kernel_resularizer_norm),activation=\"relu\"))\n",
        "                model.add(Dropout(dropout))\n",
        "                model.add(tf.keras.layers.BatchNormalization())\n",
        "                size=size//reduction_factor\n",
        "    \n",
        "        model.add(Dense(1,kernel_regularizer=regularizers.l2(3e-5),activation=\"sigmoid\",bias_initializer=bias))\n",
        "    elif targetCol==\"retweetCom\":\n",
        "        model = Sequential()\n",
        "        size=128\n",
        "        reduction_factor=2\n",
        "        shape=(inputSize,)\n",
        "        depth=3\n",
        "        dropout=0.4\n",
        "        kernel_resularizer_norm=1e-4\n",
        "        for i in range(depth):\n",
        "                    model.add(Dense(size,  input_shape=shape,kernel_regularizer=regularizers.l2(kernel_resularizer_norm),activation=\"relu\"))\n",
        "                    model.add(Dropout(dropout))\n",
        "                    model.add(tf.keras.layers.BatchNormalization())\n",
        "                    size=size//reduction_factor\n",
        "        \n",
        "        model.add(Dense(1,kernel_regularizer=regularizers.l2(3e-5),activation=\"sigmoid\",bias_initializer=bias))\n",
        "    else:\n",
        "        model = Sequential()\n",
        "        size=64\n",
        "        reduction_factor=2\n",
        "        shape=(inputSize,)\n",
        "        depth=2\n",
        "        dropout=0.4\n",
        "        kernel_resularizer_norm=1e-4\n",
        "        for i in range(depth):\n",
        "                model.add(Dense(size,  input_shape=shape,kernel_regularizer=regularizers.l2(kernel_resularizer_norm),activation=\"relu\"))\n",
        "                model.add(Dropout(dropout))\n",
        "                model.add(tf.keras.layers.BatchNormalization())\n",
        "                size=size//reduction_factor\n",
        "    \n",
        "        model.add(Dense(1,kernel_regularizer=regularizers.l2(3e-5),activation=\"sigmoid\",bias_initializer=bias))\n",
        "    return model"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8oP0ojk8EQ0"
      },
      "source": [
        "import copy\n",
        "def computeLast(arr,last):\n",
        "    return last.predict(arr)\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "def trainModel(train,trainTarget,test,testTarget,w,BS,lr,model=None,epochs=1):\n",
        "    if model==None:\n",
        "        count=len(train[0])\n",
        "        unique, counts = np.unique(trainTarget, return_counts=True)\n",
        "        if unique[0]==0:\n",
        "            res=counts[1]/counts[0]\n",
        "        else:\n",
        "            res=counts[0]/counts[1]\n",
        "        initial_bias=np.log(res)\n",
        "        model=buildModel(count,bias=initial_bias)\n",
        "    loss=create_weighted_binary_crossentropy(1,w)\n",
        "    model.compile(loss=loss, metrics=[tf.keras.metrics.AUC(name=\"PRAUC\", curve='PR'),\"accuracy\"],optimizer=tf.keras.optimizers.Adam(learning_rate=lr))\n",
        "    print(f\"w:{w} BS:{BS} lr:{lr}\")\n",
        "    \n",
        " \n",
        "    #loss= tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    #loss= tf.keras.losses.MeanSquaredError() 5.292315388887181\n",
        "    \n",
        "    model.fit(train,trainTarget,epochs=epochs,batch_size=BS)\n",
        "    prediction=model.predict(test)\n",
        "    calculate_metrics(prediction,testTarget)\n",
        "    return model"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIu40b8X8EQ2"
      },
      "source": [
        "##Chunk wise download and work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_1A25Dl8EQ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41cb7c5d-421f-477a-e856-eabc174e5952"
      },
      "source": [
        "import os \n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "gc.collect()\n",
        "model=None\n",
        "alpha=0.92\n",
        "w=1.2\n",
        "BS=64\n",
        "beta=0.95\n",
        "lr=0.0001\n",
        "epochs=1\n",
        "with open(\"training_urls.txt\") as doc:\n",
        "    index=0\n",
        "    for line in doc:#tqdm(doc, position=0, leave=True):\n",
        "        #skip lzo.index files (odd row number)\n",
        "        if index%2==1:\n",
        "            index+=1\n",
        "            continue\n",
        "        os.system(f\"aria2c '{line}'\")\n",
        "        partName=str(index//2)\n",
        "        print(partName)\n",
        "        partName=\"0\"*(5-len(partName))+partName\n",
        "        \n",
        "        os.system(f'lzop -x part-{partName}.lzo')\n",
        "        os.system(f'mv part-{partName} train.csv')\n",
        "        os.system(f\"rm part-{partName}.lzo\")\n",
        "        train,trainTarget,test,testTarget=computeTrainTest()\n",
        "        if model==None:\n",
        "            model=trainModel(train,trainTarget,test,testTarget,w,BS,lr,epochs=epochs)\n",
        "        else:\n",
        "            model=trainModel(train,trainTarget,test,testTarget,w,BS,lr,model=model,epochs=epochs)\n",
        "        if ((index//2)%30)==29:\n",
        "            if BS!=1:\n",
        "                BS=BS//2\n",
        "        w*=alpha\n",
        "        if w<1:\n",
        "            w=1\n",
        "        if w>10:\n",
        "            w=10\n",
        "        lr*=beta\n",
        "        os.system(\"rm train.csv\")\n",
        "        os.system(f\"rm part-{partName}.lzo.aria2\")\n",
        "        os.system(f\"rm part-{partName}.lzo.index\")\n",
        "        index+=1\n",
        "        gc.collect()\n",
        "os.system(\"mv ./temp.txt ./dictionary.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "here\n",
            "w:1 BS:64 lr:2.2593554099256555e-05\n",
            "40650/40650 [==============================] - 120s 3ms/step - loss: 0.6189 - PRAUC: 0.5638 - accuracy: 0.6545\n",
            "rce=8.671870882838029\n",
            "AP=0.5800484300174088\n",
            "30\n",
            "here\n",
            "w:1 BS:32 lr:2.1463876394293726e-05\n",
            "81261/81261 [==============================] - 231s 3ms/step - loss: 0.6199 - PRAUC: 0.5622 - accuracy: 0.6542\n",
            "rce=8.51733074699581\n",
            "AP=0.5793605137292586\n",
            "31\n",
            "here\n",
            "w:1 BS:32 lr:2.039068257457904e-05\n",
            "10688/81240 [==>...........................] - ETA: 3:18 - loss: 0.6215 - PRAUC: 0.5623 - accuracy: 0.6520"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zQtZzSziBTB"
      },
      "source": [
        "model.save(f\"baseNet-{targetCol}.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIimYM6Qi7TV"
      },
      "source": [
        "import json\n",
        "with open('summary.json', 'w') as fp:\n",
        "    json.dump(summary, fp)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}